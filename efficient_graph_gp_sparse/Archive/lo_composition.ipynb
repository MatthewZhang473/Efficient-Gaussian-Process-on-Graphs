{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171ca642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to your sparse_lo module\n",
    "sys.path.append('/Users/matthew/Documents/Efficient Gaussian Process on Graphs/Efficient_Gaussian_Process_On_Graphs/efficient_graph_gp_sparse/utils_sparse')\n",
    "\n",
    "# Import your SparseLinearOperator\n",
    "from sparse_lo import SparseLinearOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1362c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a simple sparse CSR tensor...\n",
      "Sparse CSR tensor:\n",
      "tensor(crow_indices=tensor([0, 2, 3, 4, 6]),\n",
      "       col_indices=tensor([0, 2, 1, 3, 0, 3]),\n",
      "       values=tensor([1., 2., 3., 4., 5., 6.]), size=(4, 4), nnz=6,\n",
      "       layout=torch.sparse_csr)\n",
      "\n",
      "Dense version:\n",
      "tensor([[1., 0., 2., 0.],\n",
      "        [0., 3., 0., 0.],\n",
      "        [0., 0., 0., 4.],\n",
      "        [5., 0., 0., 6.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/vn4885796ql7_mlpq_dc1n9r0000gn/T/ipykernel_15729/825765867.py:16: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  sparse_csr = sparse_coo.to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "# Create a simple test sparse matrix\n",
    "print(\"Creating a simple sparse CSR tensor...\")\n",
    "\n",
    "# Create a 4x4 sparse matrix with some non-zero elements\n",
    "# Matrix structure:\n",
    "# [1, 0, 2, 0]\n",
    "# [0, 3, 0, 0] \n",
    "# [0, 0, 0, 4]\n",
    "# [5, 0, 0, 6]\n",
    "\n",
    "indices = torch.tensor([[0, 0, 1, 2, 3, 3], [0, 2, 1, 3, 0, 3]])  # row, col indices\n",
    "values = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "size = (4, 4)\n",
    "\n",
    "sparse_coo = torch.sparse_coo_tensor(indices, values, size)\n",
    "sparse_csr = sparse_coo.to_sparse_csr()\n",
    "\n",
    "print(f\"Sparse CSR tensor:\\n{sparse_csr}\")\n",
    "print(f\"\\nDense version:\\n{sparse_csr.to_dense()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1277cb1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sparse CSR tensors do not have strides",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x_1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43msparse_csr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sparse CSR tensors do not have strides"
     ]
    }
   ],
   "source": [
    "x_1 = [0,1,2,3]\n",
    "\n",
    "sparse_csr[x_1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c166f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparseLinearOperator...\n",
      "Operator size: torch.Size([4, 4])\n",
      "Operator shape: torch.Size([4, 4])\n",
      "Number of non-zero elements: 6\n"
     ]
    }
   ],
   "source": [
    "# Create SparseLinearOperator and test basic properties\n",
    "print(\"Creating SparseLinearOperator...\")\n",
    "\n",
    "sparse_lo = SparseLinearOperator(sparse_csr)\n",
    "\n",
    "print(f\"Operator size: {sparse_lo.size()}\")\n",
    "print(f\"Operator shape: {sparse_lo.shape}\")\n",
    "print(f\"Number of non-zero elements: {sparse_csr._nnz()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "762c660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_lo_1 = SparseLinearOperator(sparse_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91722e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sum([3*sparse_lo_1, sparse_lo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13f79f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Documents/Efficient Gaussian Process on Graphs/Efficient_Gaussian_Process_On_Graphs/venv/lib/python3.11/site-packages/linear_operator/utils/sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "/Users/matthew/Documents/Efficient Gaussian Process on Graphs/Efficient_Gaussian_Process_On_Graphs/venv/lib/python3.11/site-packages/linear_operator/utils/sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:656.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 28.,  24.,  64., 116.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[x_1, :] @ random_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2fe1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sparse_lo[x_1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c46b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing if z[x_1, :] materializes the matrix ===\n",
      "Random vector: tensor([-2.2472, -1.2159, -0.3257,  0.5080])\n",
      "Original z type: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "Original z shape: torch.Size([4, 4])\n",
      "Is z a LinearOperator? True\n",
      "\n",
      "Testing indexing with x_1 = [0, 1, 2, 3]\n",
      "After indexing - type: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "After indexing - shape: torch.Size([4, 4])\n",
      "Is indexed_z still a LinearOperator? True\n",
      "✅ Still a linear operator: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "Result: tensor([-11.5942, -14.5903,   8.1276, -32.7518])\n",
      "\n",
      "=== Memory Usage Comparison ===\n",
      "Indexed result is still a linear operator\n",
      "\n",
      "=== Comparison with direct sparse tensor indexing ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sparse CSR tensors do not have strides",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Test with direct sparse tensor for comparison\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Comparison with direct sparse tensor indexing ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m sparse_indexed \u001b[38;5;241m=\u001b[39m \u001b[43msparse_csr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirect sparse indexing type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sparse_indexed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirect sparse indexing is_sparse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_indexed\u001b[38;5;241m.\u001b[39mis_sparse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sparse CSR tensors do not have strides"
     ]
    }
   ],
   "source": [
    "# Test if indexing materializes the SumLinearOperator\n",
    "print(\"=== Testing if z[x_1, :] materializes the matrix ===\")\n",
    "\n",
    "# First, define random_vector (which is missing in your code)\n",
    "random_vector = torch.randn(4)\n",
    "print(f\"Random vector: {random_vector}\")\n",
    "\n",
    "# Create the sum operator\n",
    "z = sum([3*sparse_lo_1, sparse_lo])\n",
    "print(f\"Original z type: {type(z)}\")\n",
    "print(f\"Original z shape: {z.shape}\")\n",
    "\n",
    "# Check if z is still a linear operator\n",
    "from linear_operator.operators import LinearOperator\n",
    "print(f\"Is z a LinearOperator? {isinstance(z, LinearOperator)}\")\n",
    "\n",
    "# Test indexing and check what happens\n",
    "x_1 = [0, 1, 2, 3]\n",
    "print(f\"\\nTesting indexing with x_1 = {x_1}\")\n",
    "\n",
    "try:\n",
    "    # Perform the indexing operation\n",
    "    indexed_z = z[x_1, :]\n",
    "    \n",
    "    print(f\"After indexing - type: {type(indexed_z)}\")\n",
    "    print(f\"After indexing - shape: {indexed_z.shape}\")\n",
    "    print(f\"Is indexed_z still a LinearOperator? {isinstance(indexed_z, LinearOperator)}\")\n",
    "    \n",
    "    # Check if it's a dense tensor (BAD - means materialized)\n",
    "    if isinstance(indexed_z, torch.Tensor):\n",
    "        print(f\"❌ MATERIALIZED! Became a torch.Tensor\")\n",
    "        print(f\"Is sparse tensor? {indexed_z.is_sparse}\")\n",
    "        if not indexed_z.is_sparse:\n",
    "            print(f\"❌ DENSE TENSOR - Memory usage will be high!\")\n",
    "        else:\n",
    "            print(f\"✅ Still sparse tensor\")\n",
    "    else:\n",
    "        print(f\"✅ Still a linear operator: {type(indexed_z)}\")\n",
    "    \n",
    "    # Test the matrix-vector multiplication\n",
    "    result = indexed_z @ random_vector\n",
    "    print(f\"Result: {result}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during operation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Compare memory usage\n",
    "print(f\"\\n=== Memory Usage Comparison ===\")\n",
    "\n",
    "# Check memory usage of original sum operator\n",
    "if hasattr(z, 'sparse_csr_tensor'):\n",
    "    original_nnz = z.sparse_csr_tensor._nnz()\n",
    "    print(f\"Original sum operator non-zeros: {original_nnz}\")\n",
    "\n",
    "# Check memory usage after indexing\n",
    "try:\n",
    "    indexed_z = z[x_1, :]\n",
    "    if isinstance(indexed_z, torch.Tensor):\n",
    "        if indexed_z.is_sparse:\n",
    "            indexed_nnz = indexed_z._nnz()\n",
    "            print(f\"Indexed sparse tensor non-zeros: {indexed_nnz}\")\n",
    "        else:\n",
    "            total_elements = indexed_z.numel()\n",
    "            print(f\"❌ Dense tensor total elements: {total_elements}\")\n",
    "            print(f\"Memory usage increased by factor: {total_elements / original_nnz:.2f}\")\n",
    "    else:\n",
    "        print(f\"Indexed result is still a linear operator\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Test with direct sparse tensor for comparison\n",
    "print(f\"\\n=== Comparison with direct sparse tensor indexing ===\")\n",
    "sparse_indexed = sparse_csr[x_1, :]\n",
    "print(f\"Direct sparse indexing type: {type(sparse_indexed)}\")\n",
    "print(f\"Direct sparse indexing is_sparse: {sparse_indexed.is_sparse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "495f021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Matrix-Vector Operations ===\n",
      "Before @ operation - z type: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "After z @ vector - result type: <class 'torch.Tensor'>\n",
      "Result: tensor([  1.7275,   9.7564,   5.1427, -19.6442])\n",
      "\n",
      "=== Testing Matrix-Matrix Operations ===\n",
      "After z @ matrix - result type: <class 'torch.Tensor'>\n",
      "Result shape: torch.Size([4, 3])\n",
      "\n",
      "=== Testing Transpose Operations ===\n",
      "After transpose - type: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "After transpose @ vector - result type: <class 'torch.Tensor'>\n",
      "\n",
      "=== Testing with Larger Matrices ===\n",
      "Large matrix z type: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "Large matrix z shape: torch.Size([1000, 1000])\n",
      "Large matrix indexing - type: <class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n",
      "Large matrix indexing - shape: torch.Size([100, 1000])\n",
      "✅ Large matrix indexing still lazy\n"
     ]
    }
   ],
   "source": [
    "# Test matrix-vector multiplication directly\n",
    "print(\"\\n=== Testing Matrix-Vector Operations ===\")\n",
    "\n",
    "# Test if @ operator materializes\n",
    "z = sum([3*sparse_lo_1, sparse_lo])\n",
    "random_vector = torch.randn(4)\n",
    "\n",
    "print(f\"Before @ operation - z type: {type(z)}\")\n",
    "\n",
    "# This is what happens in your kernel during training\n",
    "result = z @ random_vector\n",
    "print(f\"After z @ vector - result type: {type(result)}\")\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Test matrix-matrix multiplication (this might be the problem)\n",
    "print(\"\\n=== Testing Matrix-Matrix Operations ===\")\n",
    "random_matrix = torch.randn(4, 3)\n",
    "\n",
    "try:\n",
    "    result_mm = z @ random_matrix\n",
    "    print(f\"After z @ matrix - result type: {type(result_mm)}\")\n",
    "    print(f\"Result shape: {result_mm.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Matrix-matrix multiplication failed: {e}\")\n",
    "\n",
    "# Test transpose operations\n",
    "print(\"\\n=== Testing Transpose Operations ===\")\n",
    "try:\n",
    "    z_t = z.t()\n",
    "    print(f\"After transpose - type: {type(z_t)}\")\n",
    "    result_t = z_t @ random_vector\n",
    "    print(f\"After transpose @ vector - result type: {type(result_t)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Transpose operation failed: {e}\")\n",
    "\n",
    "# Test what happens with larger matrices (closer to your real scenario)\n",
    "print(\"\\n=== Testing with Larger Matrices ===\")\n",
    "# Create a larger sparse matrix to simulate your real case\n",
    "large_indices = torch.tensor([[0, 1, 2, 100, 200], [50, 60, 70, 150, 250]])\n",
    "large_values = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "large_size = (1000, 1000)  # Much larger\n",
    "\n",
    "large_sparse = torch.sparse_coo_tensor(large_indices, large_values, large_size).to_sparse_csr()\n",
    "large_slo = SparseLinearOperator(large_sparse)\n",
    "large_z = sum([3*large_slo, large_slo])\n",
    "\n",
    "print(f\"Large matrix z type: {type(large_z)}\")\n",
    "print(f\"Large matrix z shape: {large_z.shape}\")\n",
    "\n",
    "# Test indexing on large matrix\n",
    "test_indices = list(range(100))  # First 100 rows\n",
    "try:\n",
    "    large_indexed = large_z[test_indices, :]\n",
    "    print(f\"Large matrix indexing - type: {type(large_indexed)}\")\n",
    "    print(f\"Large matrix indexing - shape: {large_indexed.shape}\")\n",
    "    \n",
    "    # Test if this triggers materialization\n",
    "    if isinstance(large_indexed, torch.Tensor) and not large_indexed.is_sparse:\n",
    "        print(f\"❌ LARGE MATRIX MATERIALIZED!\")\n",
    "        print(f\"Total elements: {large_indexed.numel()}\")\n",
    "    else:\n",
    "        print(f\"✅ Large matrix indexing still lazy\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Large matrix indexing failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0480d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing addition of SparseLinearOperators...\n",
      "Sum operator size: torch.Size([4, 4])\n",
      "Sum operator shape: torch.Size([4, 4])\n",
      "Testing multiplication of SparseLinearOperators...\n",
      "Product operator size: torch.Size([4, 4])\n",
      "Product operator shape: torch.Size([4, 4])\n",
      "Operating on vector tensor([1., 2., 3., 4.], requires_grad=True)...\n",
      "Result of operation: tensor([ 39.,  18., 116., 209.], grad_fn=<MatmulBackward>)\n",
      "Testing autograd with SparseLinearOperator...\n",
      "Gradient of the vector: tensor([56.,  9., 12., 68.])\n",
      "Expected gradient: tensor([56.,  9., 12., 68.])\n",
      "Gradient matches expected: True\n"
     ]
    }
   ],
   "source": [
    "slo1 = SparseLinearOperator(sparse_csr)\n",
    "slo2 = SparseLinearOperator(sparse_csr)\n",
    "# Test addition\n",
    "print(\"Testing addition of SparseLinearOperators...\")\n",
    "slo_sum = slo1 + slo2\n",
    "print(f\"Sum operator size: {slo_sum.size()}\")\n",
    "print(f\"Sum operator shape: {slo_sum.shape}\")\n",
    "\n",
    "# test multiplication\n",
    "print(\"Testing multiplication of SparseLinearOperators...\")\n",
    "slo_prod = slo1 @ slo2\n",
    "print(f\"Product operator size: {slo_prod.size()}\")\n",
    "print(f\"Product operator shape: {slo_prod.shape}\")\n",
    "\n",
    "# Create vector with gradient tracking enabled\n",
    "vec = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "print(f\"Operating on vector {vec}...\")\n",
    "result = slo_prod @ vec\n",
    "print(f\"Result of operation: {result}\") \n",
    "\n",
    "# Test autograd\n",
    "print(\"Testing autograd with SparseLinearOperator...\")\n",
    "# Clear any existing gradients\n",
    "if vec.grad is not None:\n",
    "    vec.grad.zero_()\n",
    "\n",
    "result = slo_prod @ vec\n",
    "result.sum().backward()\n",
    "print(f\"Gradient of the vector: {vec.grad}\")\n",
    "\n",
    "# Verify the autograd functionality by calculating the gradient manually\n",
    "# For (A @ B) @ vec, gradient w.r.t vec is (A @ B)^T @ ones_vector\n",
    "# Since we're taking sum().backward(), the upstream gradient is a vector of ones\n",
    "with torch.no_grad():\n",
    "    # Get the dense matrices for manual calculation\n",
    "    A_dense = slo1.sparse_csr_tensor.to_dense()\n",
    "    B_dense = slo2.sparse_csr_tensor.to_dense()\n",
    "    AB_dense = A_dense @ B_dense\n",
    "    ones_vector = torch.ones_like(result)\n",
    "    expected_grad = AB_dense.t() @ ones_vector\n",
    "\n",
    "print(f\"Expected gradient: {expected_grad}\")\n",
    "print(f\"Gradient matches expected: {torch.allclose(vec.grad, expected_grad, atol=1e-6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13480bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
