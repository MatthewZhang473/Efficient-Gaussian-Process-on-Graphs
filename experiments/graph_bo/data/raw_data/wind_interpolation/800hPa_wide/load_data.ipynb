{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21340c3c",
   "metadata": {},
   "source": [
    "# Wind Data Loading and Preprocessing - 800hPa (Downsampled)\n",
    "\n",
    "This notebook loads and preprocesses ERA5 wind data at 800hPa pressure level with **10x downsampling** for efficient graph-based wind interpolation. The downsampling reduces computational load from ~1M nodes to ~10K nodes while maintaining essential spatial patterns.\n",
    "\n",
    "**Key Features:**\n",
    "- 10x downsampling in both lat/lon directions (721×1440 → 73×144 grid)\n",
    "- Consistent node indexing for graph-based methods\n",
    "- Normalized wind speeds (mean=0, std=1) \n",
    "- Aeolus satellite track for training data\n",
    "- Sparse adjacency matrix with geodesic edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5510fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Pressure Level: 800 hPa\n",
      "  Downsampling Factor: 10x\n",
      "  Expected grid reduction: ~100x fewer nodes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from datetime import datetime, timedelta\n",
    "from skyfield.api import load, EarthSatellite, wgs84, utc\n",
    "\n",
    "# Configuration parameters\n",
    "DOWNSAMPLE_FACTOR = 10      # Downsample by factor of 10 in both lat/lon directions\n",
    "PRESSURE_LEVEL = 800        # hPa pressure level\n",
    "PRESSURE_INDEX = 1          # 800hPa is index 1 in the dataset\n",
    "DATA_FILE = '../8176c14c59fd8dc32a74a89b926cb7fd.nc'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Pressure Level: {PRESSURE_LEVEL} hPa\")\n",
    "print(f\"  Downsampling Factor: {DOWNSAMPLE_FACTOR}x\")\n",
    "print(f\"  Expected grid reduction: ~100x fewer nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66013ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables: ['number', 'valid_time', 'pressure_level', 'latitude', 'longitude', 'expver', 'u', 'v']\n",
      "\n",
      "Data shapes:\n",
      "  Latitude: (721,)\n",
      "  Longitude: (1440,)\n",
      "  U component: (721, 1440)\n",
      "  V component: (721, 1440)\n",
      "  Original grid size: 721 × 1440 = 1,038,240 points\n",
      "\n",
      "Data shapes:\n",
      "  Latitude: (721,)\n",
      "  Longitude: (1440,)\n",
      "  U component: (721, 1440)\n",
      "  V component: (721, 1440)\n",
      "  Original grid size: 721 × 1440 = 1,038,240 points\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD NETCDF WIND DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load the NetCDF dataset\n",
    "dataset = Dataset(DATA_FILE, mode=\"r\")\n",
    "print(\"Available variables:\", list(dataset.variables.keys()))\n",
    "\n",
    "# Load coordinate arrays\n",
    "lat = dataset.variables[\"latitude\"][:]      # shape (721,)\n",
    "lon = dataset.variables[\"longitude\"][:]     # shape (1440,)\n",
    "\n",
    "# Load wind components (eastward and northward)\n",
    "# Dimensions: (valid_time=12, pressure_level=3, latitude=721, longitude=1440)\n",
    "u = dataset.variables[\"u\"][:]   # eastward wind\n",
    "v = dataset.variables[\"v\"][:]   # northward wind\n",
    "\n",
    "# Extract wind components for 800hPa pressure level\n",
    "u_800 = u[0, PRESSURE_INDEX, :, :]  # shape (721, 1440)\n",
    "v_800 = v[0, PRESSURE_INDEX, :, :]  # shape (721, 1440)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Latitude: {lat.shape}\")\n",
    "print(f\"  Longitude: {lon.shape}\")\n",
    "print(f\"  U component: {u_800.shape}\")\n",
    "print(f\"  V component: {v_800.shape}\")\n",
    "print(f\"  Original grid size: {lat.shape[0]} × {lon.shape[0]} = {lat.shape[0] * lon.shape[0]:,} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e6559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling Results:\n",
      "  Original grid: 721 × 1440 = 1,038,240 points\n",
      "  Downsampled grid: 73 × 144 = 10,512 points\n",
      "  Reduction factor: 98.8x smaller\n",
      "  Resolution: -2.50° lat × 2.50° lon\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLY DOWNSAMPLING\n",
    "# =============================================================================\n",
    "\n",
    "def downsample_grid_data(lat, lon, u_data, v_data, factor=10):\n",
    "    \"\"\"Downsample the lat/lon grid and corresponding data by a given factor\"\"\"\n",
    "    lat_down = lat[::factor]\n",
    "    lon_down = lon[::factor]\n",
    "    u_down = u_data[::factor, ::factor]\n",
    "    v_down = v_data[::factor, ::factor]\n",
    "    \n",
    "    print(f\"Downsampling Results:\")\n",
    "    print(f\"  Original grid: {lat.shape[0]} × {lon.shape[0]} = {len(lat) * len(lon):,} points\")\n",
    "    print(f\"  Downsampled grid: {lat_down.shape[0]} × {lon_down.shape[0]} = {len(lat_down) * len(lon_down):,} points\")\n",
    "    print(f\"  Reduction factor: {(len(lat) * len(lon)) / (len(lat_down) * len(lon_down)):.1f}x smaller\")\n",
    "    print(f\"  Resolution: {(lat_down[1] - lat_down[0]):.2f}° lat × {(lon_down[1] - lon_down[0]):.2f}° lon\")\n",
    "    \n",
    "    return lat_down, lon_down, u_down, v_down\n",
    "\n",
    "# Apply downsampling\n",
    "lat_processed, lon_processed, u_800_processed, v_800_processed = downsample_grid_data(\n",
    "    lat, lon, u_800, v_800, factor=DOWNSAMPLE_FACTOR\n",
    ")\n",
    "\n",
    "# Calculate wind speed magnitude for the downsampled data\n",
    "wind_speed_processed = np.sqrt(u_800_processed**2 + v_800_processed**2)\n",
    "\n",
    "# Save the downsampled data\n",
    "np.savez(f'wind_data_{PRESSURE_LEVEL}hPa_downsampled.npz', \n",
    "         lon=lon_processed, lat=lat_processed, \n",
    "         u=u_800_processed, v=v_800_processed, wind_speed=wind_speed_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa0e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Aeolus track: 1440 points, 1423 unique locations\n",
      "Graph: 10,512 nodes, 20,880 edges\n",
      "Training: 1,440 points, 1,423 unique nodes\n",
      "Wind normalization: mean=5.717 m/s, std=4.060 m/s\n",
      "Dataset: 10,512 total nodes, 1,423 training nodes (13.537%)\n",
      "Graph: 10,512 nodes, 20,880 edges\n",
      "Training: 1,440 points, 1,423 unique nodes\n",
      "Wind normalization: mean=5.717 m/s, std=4.060 m/s\n",
      "Dataset: 10,512 total nodes, 1,423 training nodes (13.537%)\n",
      "\n",
      "✅ All datasets saved to 'wind_data_processed_800hPa_wide.npz'\n",
      "✅ Grid: 73×144 = 10,512 nodes (downsampled 10x)\n",
      "✅ Normalized data: mean=0, std=1\n",
      "\n",
      "✅ All datasets saved to 'wind_data_processed_800hPa_wide.npz'\n",
      "✅ Grid: 73×144 = 10,512 nodes (downsampled 10x)\n",
      "✅ Normalized data: mean=0, std=1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GENERATE AEOLUS SATELLITE TRACK & BUILD GRAPH & PREPARE DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "# Define Aeolus TLE data and generate track\n",
    "line1 = \"1 43600U 18066A   21153.73585495  .00031128  00000-0  12124-3 0  9990\"\n",
    "line2 = \"2 43600  96.7150 160.8035 0006915  90.4181 269.7884 15.87015039160910\"\n",
    "\n",
    "ts = load.timescale()\n",
    "aeolus = EarthSatellite(line1, line2, \"AEOLUS\", ts)\n",
    "\n",
    "start = datetime(2019, 1, 1, 9, tzinfo=utc)\n",
    "times = [start + timedelta(minutes=i) for i in range(1440)]  # 24h, 1min intervals\n",
    "\n",
    "geocentric = aeolus.at(ts.from_datetimes(times))\n",
    "sat_lat, sat_lon = wgs84.latlon_of(geocentric)\n",
    "sat_lat, sat_lon = sat_lat.degrees, sat_lon.degrees % 360\n",
    "\n",
    "# Snap to downsampled grid\n",
    "def snap_to_grid(lat_val, lon_val, era5_lat, era5_lon):\n",
    "    i = np.abs(era5_lat - lat_val).argmin()\n",
    "    j = np.abs(era5_lon - lon_val).argmin()\n",
    "    return era5_lat[i], era5_lon[j]\n",
    "\n",
    "snapped = [snap_to_grid(phi, lam, lat_processed, lon_processed) for phi, lam in zip(sat_lat, sat_lon)]\n",
    "snap_lat, snap_lon = zip(*snapped)\n",
    "snapped_track = pd.DataFrame({\"time\": times, \"lat\": snap_lat, \"lon\": snap_lon})\n",
    "\n",
    "print(f\"Generated Aeolus track: {len(times)} points, {len(set(zip(snap_lat, snap_lon)))} unique locations\")\n",
    "\n",
    "# Build sphere grid graph utilities\n",
    "def deg2rad(x): return np.deg2rad(x)\n",
    "def sph2cart(lat_deg, lon_deg, r=1.0):\n",
    "    lat, lon = deg2rad(lat_deg), deg2rad(lon_deg)\n",
    "    x = r * np.cos(lat) * np.cos(lon)\n",
    "    y = r * np.cos(lat) * np.sin(lon)  \n",
    "    z = r * np.sin(lat)\n",
    "    return np.stack([x, y, z], axis=-1)\n",
    "\n",
    "def great_circle_distance(lat1_deg, lon1_deg, lat2_deg, lon2_deg, R=1.0):\n",
    "    lat1, lon1 = deg2rad(lat1_deg), deg2rad(lon1_deg)\n",
    "    lat2, lon2 = deg2rad(lat2_deg), deg2rad(lon2_deg)\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2.0 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "    return R * c\n",
    "\n",
    "def grid_index(i, j, n_lat, n_lon): return i * n_lon + j\n",
    "\n",
    "def build_sphere_grid_graph(lat, lon):\n",
    "    n_lat, n_lon = len(lat), len(lon)\n",
    "    Lon_grid, Lat_grid = np.meshgrid(lon, lat)\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            nid = grid_index(i, j, n_lat, n_lon)\n",
    "            for di, dj in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "                ii, jj = i + di, (j + dj) % n_lon\n",
    "                if 0 <= ii < n_lat:\n",
    "                    nid2 = grid_index(ii, jj, n_lat, n_lon)\n",
    "                    w = great_circle_distance(lat[i], lon[j], lat[ii], lon[jj])\n",
    "                    rows.append(nid)\n",
    "                    cols.append(nid2)\n",
    "                    data.append(w)\n",
    "    \n",
    "    A = sparse.coo_matrix((data, (rows, cols)), shape=(n_lat*n_lon, n_lat*n_lon))\n",
    "    A = ((A + A.T) * 0.5).tocsr()\n",
    "    return nx.from_scipy_sparse_array(A), A\n",
    "\n",
    "def nearest_node_indices_for_track(track_lat, track_lon, lat, lon):\n",
    "    lat, lon = np.asarray(lat), np.asarray(lon)\n",
    "    track_lat, track_lon = np.asarray(track_lat), np.asarray(track_lon) % 360.0\n",
    "    i_idx = np.abs(track_lat[:, None] - lat[None, :]).argmin(axis=1)\n",
    "    j_idx = np.abs(track_lon[:, None] - lon[None, :]).argmin(axis=1)\n",
    "    return i_idx * len(lon) + j_idx\n",
    "\n",
    "# Build graph and get training nodes\n",
    "G, A = build_sphere_grid_graph(lat_processed, lon_processed)\n",
    "node_ids = nearest_node_indices_for_track(snapped_track[\"lat\"].values, snapped_track[\"lon\"].values, lat_processed, lon_processed)\n",
    "\n",
    "print(f\"Graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "print(f\"Training: {len(node_ids):,} points, {len(np.unique(node_ids)):,} unique nodes\")\n",
    "\n",
    "# Prepare final datasets\n",
    "n_lat, n_lon = len(lat_processed), len(lon_processed)\n",
    "X = np.arange(n_lat * n_lon)\n",
    "y = np.zeros(n_lat * n_lon)\n",
    "coord_mapping = np.zeros((n_lat * n_lon, 2))\n",
    "\n",
    "for i in range(n_lat):\n",
    "    for j in range(n_lon):\n",
    "        node_id = i * n_lon + j\n",
    "        u_val, v_val = u_800_processed[i, j], v_800_processed[i, j]\n",
    "        y[node_id] = np.sqrt(u_val**2 + v_val**2)\n",
    "        coord_mapping[node_id] = [lat_processed[i], lon_processed[j]]\n",
    "\n",
    "# Normalize wind speeds\n",
    "y_mean, y_std = np.mean(y), np.std(y)\n",
    "y = (y - y_mean) / y_std\n",
    "\n",
    "# Training data\n",
    "X_train = np.unique(node_ids)\n",
    "y_train = y[X_train]\n",
    "\n",
    "print(f\"Wind normalization: mean={y_mean:.3f} m/s, std={y_std:.3f} m/s\")\n",
    "print(f\"Dataset: {len(X):,} total nodes, {len(X_train):,} training nodes ({len(X_train)/len(X)*100:.3f}%)\")\n",
    "\n",
    "# Save prepared datasets\n",
    "u_flat = np.zeros(n_lat * n_lon)\n",
    "v_flat = np.zeros(n_lat * n_lon)\n",
    "for i in range(n_lat):\n",
    "    for j in range(n_lon):\n",
    "        node_id = i * n_lon + j\n",
    "        u_flat[node_id] = u_800_processed[i, j]\n",
    "        v_flat[node_id] = v_800_processed[i, j]\n",
    "\n",
    "np.savez(f'wind_data_processed_{PRESSURE_LEVEL}hPa_wide.npz',\n",
    "         A_data=A.data, A_indices=A.indices, A_indptr=A.indptr, A_shape=A.shape,\n",
    "         X=X, y=y, y_mean=y_mean, y_std=y_std, X_train=X_train, y_train=y_train,\n",
    "         coord_mapping=coord_mapping, u_component=u_flat, v_component=v_flat,\n",
    "         downsample_factor=DOWNSAMPLE_FACTOR, pressure_level=PRESSURE_LEVEL)\n",
    "\n",
    "print(f\"\\n✅ All datasets saved to 'wind_data_processed_{PRESSURE_LEVEL}hPa_wide.npz'\")\n",
    "print(f\"✅ Grid: {n_lat}×{n_lon} = {n_lat*n_lon:,} nodes (downsampled {DOWNSAMPLE_FACTOR}x)\")\n",
    "print(f\"✅ Normalized data: mean=0, std=1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
