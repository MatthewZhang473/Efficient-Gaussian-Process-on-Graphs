{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import networkx as nx\n",
    "from gpflow.utilities import print_summary\n",
    "import tensorflow_probability as tfp\n",
    "import seaborn as sns\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(\"../..\")\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_graph_gp.graph_kernels import diffusion_kernel, get_normalized_laplacian, generate_noisy_samples\n",
    "from efficient_graph_gp.gpflow_kernels import GraphDiffusionKernel, GraphDiffusionFastGRFKernel, GraphDiffusionPoFMKernel, GraphGeneralPoFMKernel\n",
    "from utils import plot_network_graph, plot_gp_fit, compute_fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_graph_by_degree(num_nodes, average_degree, seed=42):\n",
    "    probability = average_degree / (num_nodes - 1)  # Calculate edge probability from average degree\n",
    "    G = nx.erdos_renyi_graph(num_nodes, probability, seed=seed, directed=False)\n",
    "    return nx.to_numpy_array(G)\n",
    "\n",
    "def gp_inference(X,Y,X_new, graph_kernel):\n",
    "    model = gpflow.models.GPR(data=(X, Y), kernel=graph_kernel, mean_function=None)\n",
    "    # model.likelihood.variance.prior = tfp.distributions.LogNormal(loc=np.log(0.07), scale=0.5)\n",
    "    gpflow.optimizers.Scipy().minimize(model.training_loss, model.trainable_variables)\n",
    "    mean, variance = model.predict_f(X_new)\n",
    "    stddev = tf.sqrt(variance)\n",
    "    return model, mean, stddev\n",
    "\n",
    "def gp_inference_fixed_noise(X,Y,X_new, graph_kernel, noise_variance):\n",
    "    model = gpflow.models.GPR(data=(X, Y), kernel=graph_kernel, mean_function=None)\n",
    "    model.likelihood.variance.assign(noise_variance)\n",
    "    gpflow.utilities.set_trainable(model.likelihood.variance, False)\n",
    "    gpflow.optimizers.Scipy().minimize(model.training_loss, model.trainable_variables)\n",
    "    mean, variance = model.predict_f(X_new)\n",
    "    stddev = tf.sqrt(variance)\n",
    "    return model, mean, stddev\n",
    "\n",
    "def lengthscale2modulator(beta, max_expansion):\n",
    "    theta = np.array([(-beta / 2) ** i / math.factorial(i) for i in range(max_expansion)])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_log_marginal_likelihood_terms(y, K, sigma_n):\n",
    "    \"\"\"\n",
    "    Compute the three parts of the Gaussian Process log marginal likelihood\n",
    "    (assuming mean = 0):\n",
    "      1) data_fit    = -1/2 * y^T (K + sigma_n^2 I)^-1 y\n",
    "      2) complexity  = -1/2 * log |K + sigma_n^2 I|\n",
    "      3) constant    = -n/2 * log(2 pi)\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    Ky = K + sigma_n**2 * np.eye(n)\n",
    "    invKy = np.linalg.inv(Ky)\n",
    "    sign, logdet = np.linalg.slogdet(Ky)\n",
    "\n",
    "    data_fit   = -0.5 * y.T  @ invKy @ y\n",
    "    complexity = -0.5 * logdet\n",
    "    constant   = -0.5 * n * np.log(2 * np.pi)\n",
    "\n",
    "    return data_fit, complexity, constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random graph\n",
    "num_nodes = 1000\n",
    "average_degree = 10\n",
    "adjacency_matrix = generate_random_graph_by_degree(num_nodes, average_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrate Noisy Samples from the Graph By Sampling from a GP\n",
    "beta_sample = 3.0\n",
    "noise_std = 0.01\n",
    "\n",
    "K_true = diffusion_kernel(adjacency_matrix, beta_sample)\n",
    "Y_noisy = generate_noisy_samples(K_true, noise_std=noise_std)\n",
    "X = tf.convert_to_tensor(np.arange(num_nodes, dtype=np.float64).reshape(-1, 1))\n",
    "X_new = tf.convert_to_tensor(np.arange(num_nodes, dtype=np.float64).reshape(-1, 1))\n",
    "Y = tf.reshape(tf.convert_to_tensor(Y_noisy, dtype=tf.float64), (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GP Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EXPANSION = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Analytical Ground Truth PoFM Hyperparameter $\\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.    -1.5    1.125]\n"
     ]
    }
   ],
   "source": [
    "theta_0 = lengthscale2modulator(beta_sample, MAX_EXPANSION)\n",
    "print(theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Best Fit PoFM Hyperparameter using Exact Diffusion Kernel $\\theta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕\n",
      "│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │\n",
      "╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡\n",
      "│ GPR.kernel.beta         │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 3.05433 │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤\n",
      "│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ False       │ ()      │ float64 │ 0.0001  │\n",
      "╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "exact_kernel = GraphDiffusionKernel(adjacency_matrix, beta=0.5) # The initial beta value for optimizaiton\n",
    "model, mean, stddev = gp_inference_fixed_noise(X, Y, X_new, exact_kernel, noise_variance=noise_std**2)\n",
    "print_summary(model)\n",
    "learned_beta_exact = model.kernel.beta.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -1.52716686  1.16611931]\n",
      "103.35114714879683\n"
     ]
    }
   ],
   "source": [
    "theta_1 = lengthscale2modulator(learned_beta_exact, MAX_EXPANSION)\n",
    "print(theta_1)\n",
    "print(model.log_marginal_likelihood().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Best Fit PoFM Hyperparameter using General PoFM Kernel $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════════════════════════════════╕\n",
      "│ name                        │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │ value                                 │\n",
      "╞═════════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════════════════════════════════╡\n",
      "│ GPR.kernel.modulator_vector │ Parameter │ Identity         │         │ True        │ (3,)    │ float64 │ [-0.88681834  0.97648526 -0.30462473] │\n",
      "├─────────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────────────────────────────────┤\n",
      "│ GPR.likelihood.variance     │ Parameter │ Softplus + Shift │         │ False       │ ()      │ float64 │ 0.00010000000000000011                │\n",
      "╘═════════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "general_pofm_kernel = GraphGeneralPoFMKernel(adjacency_matrix, max_walk_length=MAX_EXPANSION)\n",
    "model, mean, stddev = gp_inference_fixed_noise(X, Y, X_new, general_pofm_kernel, noise_variance=noise_std**2)\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.88681834  0.97648526 -0.30462473]\n",
      "103.02230154786344\n"
     ]
    }
   ],
   "source": [
    "theta_hat = model.kernel.modulator_vector.numpy()\n",
    "print(theta_hat)\n",
    "print(model.log_marginal_likelihood().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Grid Search to Visualize the Posterior Distribution of $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_diffusion(adjacency_matrix, X, Y, beta_values, noise_variance):\n",
    "    ml_values = []\n",
    "    for beta in tqdm(beta_values, desc=\"Evaluating ML over beta (Exact Diffusion)\"):\n",
    "        kernel = GraphDiffusionKernel(adjacency_matrix, beta=beta)\n",
    "        model = gpflow.models.GPR(data=(X, Y), kernel=kernel, mean_function=None)\n",
    "        model.likelihood.variance.assign(noise_variance)\n",
    "        ml_values.append(np.exp(model.log_marginal_likelihood().numpy()))\n",
    "    return ml_values\n",
    "\n",
    "def evaluate_ml_constrained_pofm(adjacency_matrix, X, Y, beta_values, noise_variance):\n",
    "    ml_values = []\n",
    "    for beta in tqdm(beta_values, desc=\"Evaluating ML over beta (PoFM)\"):\n",
    "        theta = lengthscale2modulator(beta, MAX_EXPANSION)\n",
    "        kernel = GraphGeneralPoFMKernel(adjacency_matrix, max_walk_length=MAX_EXPANSION, modulator_vector=theta)\n",
    "        model = gpflow.models.GPR(data=(X, Y), kernel=kernel, mean_function=None)\n",
    "        model.likelihood.variance.assign(noise_variance)\n",
    "        ml_values.append(np.exp(model.log_marginal_likelihood().numpy()))\n",
    "    return ml_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the marginal likelihood distribution to verify the correctness of the optimization\n",
    "\n",
    "# beta_values = np.linspace(0.1, 5, 300)\n",
    "\n",
    "# ml_values_diff = evaluate_ml_diffusion(adjacency_matrix, X, Y, beta_values, noise_std**2)\n",
    "# ml_values_diff_normalized = ml_values_diff / np.sum(ml_values_diff)\n",
    "\n",
    "# ml_values_constrained_pofm = evaluate_ml_constrained_pofm(adjacency_matrix, X, Y, beta_values, noise_std**2)\n",
    "# ml_values_constrained_pofm_normalized = ml_values_constrained_pofm / np.sum(ml_values_constrained_pofm)\n",
    "\n",
    "# plt.plot(beta_values, ml_values_diff_normalized, label=\"Diffusion Kernel\")\n",
    "# plt.plot(beta_values, ml_values_constrained_pofm_normalized, label=\"Constrained PoFM Kernel\")\n",
    "# plt.xlabel(\"Beta\")\n",
    "# plt.ylabel(\"Normalized Marginal Likelihood\")\n",
    "# plt.title(\"Marginal Likelihood v.s. Beta\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Break Down the Marginal Likelihood\n",
    "\n",
    "\n",
    "My guess is that the PoFM model gives better data fit term, but with larger (potentially much larger) model complexity. We can think about how to perhaps penalize the complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_kernel = GraphDiffusionKernel(adjacency_matrix, beta=beta_sample) # The initial beta value for optimizaiton\n",
    "model = gpflow.models.GPR(data=(X, Y), kernel=ground_truth_kernel, mean_function=None)\n",
    "model.likelihood.variance.assign(noise_std**2)\n",
    "K_ground_truth = model.kernel.K(X).numpy()\n",
    "sigma_ground_truth = np.sqrt(model.likelihood.variance.numpy())\n",
    "data_fit, complexity, constant = gp_log_marginal_likelihood_terms(Y_noisy, K_ground_truth, sigma_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting the Log Marginal Likelihood Terms of the Ground Truth Kernel\n",
      "data fit:  -476.97826229003465\n",
      "negative complexity:  1498.4804831356607\n",
      "noise std:  0.010000000000000005\n",
      "constant:  -918.9385332046727\n",
      "log marginal likelihood:  102.56368764095339\n"
     ]
    }
   ],
   "source": [
    "print('Inspecting the Log Marginal Likelihood Terms of the Ground Truth Kernel')\n",
    "print('data fit: ', data_fit.squeeze())\n",
    "print('negative complexity: ', complexity)\n",
    "print('noise std: ', sigma_ground_truth)\n",
    "print('constant: ', constant)\n",
    "print('log marginal likelihood: ', data_fit.squeeze() + complexity + constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_kernel = GraphDiffusionKernel(adjacency_matrix, beta=0.5) # The initial beta value for optimizaiton\n",
    "model, _, _ = gp_inference_fixed_noise(X, Y, X_new, exact_kernel, noise_variance=noise_std**2)\n",
    "K_exact = model.kernel.K(X).numpy()\n",
    "sigma_exact = np.sqrt(model.likelihood.variance.numpy())\n",
    "data_fit, complexity, constant = gp_log_marginal_likelihood_terms(Y_noisy, K_exact, sigma_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting the Log Marginal Likelihood Terms of the Exact Diffusion Kernel\n",
      "data fit:  -503.2503033257732\n",
      "negative complexity:  1525.5399836792446\n",
      "noise std:  0.010000000000000005\n",
      "constant:  -918.9385332046727\n",
      "log marginal likelihood:  103.35114714879876\n"
     ]
    }
   ],
   "source": [
    "print('Inspecting the Log Marginal Likelihood Terms of the Exact Diffusion Kernel')\n",
    "print('data fit: ', data_fit.squeeze())\n",
    "print('negative complexity: ', complexity)\n",
    "print('noise std: ', sigma_exact)\n",
    "print('constant: ', constant)\n",
    "print('log marginal likelihood: ', data_fit.squeeze() + complexity + constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pofm_kernel = GraphGeneralPoFMKernel(adjacency_matrix, max_walk_length=MAX_EXPANSION)\n",
    "model, _, _, = gp_inference_fixed_noise(X, Y, X_new, general_pofm_kernel, noise_variance=noise_std**2)\n",
    "K_pofm = model.kernel.K(X).numpy()\n",
    "sigma_pofm = np.sqrt(model.likelihood.variance.numpy())\n",
    "data_fit, complexity, constant = gp_log_marginal_likelihood_terms(Y_noisy, K_pofm, sigma_pofm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting the Log Marginal Likelihood Terms of the PoFM Kernel\n",
      "data fit:  -500.00440856313617\n",
      "negative complexity:  1521.9652433156712\n",
      "noise std:  0.010000000000000005\n",
      "constant:  -918.9385332046727\n",
      "log marginal likelihood:  103.0223015478623\n"
     ]
    }
   ],
   "source": [
    "print('Inspecting the Log Marginal Likelihood Terms of the PoFM Kernel')\n",
    "print('data fit: ', data_fit.squeeze())\n",
    "print('negative complexity: ', complexity)\n",
    "print('noise std: ', sigma_pofm)\n",
    "print('constant: ', constant)\n",
    "print('log marginal likelihood: ', data_fit.squeeze() + complexity + constant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
