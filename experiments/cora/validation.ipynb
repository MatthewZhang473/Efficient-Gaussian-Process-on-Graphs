{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c256f6",
   "metadata": {},
   "source": [
    "# Cora Classification with SVGP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e82a520",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60953924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import networkx as nx\n",
    "from gpflow.utilities import print_summary\n",
    "import tensorflow_probability as tfp\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(\"../..\")\n",
    "sys.path.append(project_root)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from efficient_graph_gp.graph_kernels import get_normalized_laplacian\n",
    "from efficient_graph_gp.gpflow_kernels import GraphDiffusionFastGRFKernel, GraphDiffusionPoFMKernel, GraphDiffusionKernel, GraphGeneralPoFMKernel, GraphGeneralFastGRFKernel\n",
    "from utils import compute_fro\n",
    "from cora_utils.preprocessing import load_PEMS, load_cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Hyperparameters\n",
    "MAX_WALK_LENGTH = 3\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "M = 140 # Number of training points\n",
    "batch_size = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "num_eigenpairs = 500\n",
    "dataset = 'cora'\n",
    "cls_number = 7\n",
    "train_num = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89807031",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, data_train, data_test = load_cora(num_train=train_num, num_test=1000)\n",
    "adjacency_matrix = nx.to_numpy_array(G) \n",
    "x_train, y_train = data_train\n",
    "x_test, y_test = data_test\n",
    "Z = x_train[np.random.choice(train_num, M, replace=False)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abfe8c",
   "metadata": {},
   "source": [
    "### Split the 140 labeled nodes into 112 train / 28 val (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train: shape (140, 1)    node indices for the labeled set\n",
    "# y_train: shape (140,)      corresponding labels (0..6)\n",
    "\n",
    "# Stratified 80/20 split → 112 inner‐train, 28 validation\n",
    "X_all = x_train   # (140, 1)\n",
    "Y_all = y_train   # (140,)\n",
    "\n",
    "X_fit, X_val, Y_fit, Y_val = train_test_split(\n",
    "    X_all,\n",
    "    Y_all,\n",
    "    test_size=0.2,        # 20% of 140 = 28 nodes for validation\n",
    "    stratify=Y_all,       # keep class proportions\n",
    "    random_state=123      # for reproducibility\n",
    ")\n",
    "\n",
    "print(\"X_fit  shape:\", X_fit.shape, \"  Y_fit shape:\", Y_fit.shape)\n",
    "print(\"X_val  shape:\", X_val.shape, \"  Y_val shape:\", Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bde3f",
   "metadata": {},
   "source": [
    "## 1. PoFM Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675343f",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WALK_LENGTH = 3\n",
    "graph_kernel = GraphGeneralPoFMKernel(adjacency_matrix,\n",
    "                                      max_walk_length=MAX_WALK_LENGTH,\n",
    "                                      normalize_laplacian=True)\n",
    "likelihood = gpflow.likelihoods.MultiClass(num_classes=cls_number)\n",
    "\n",
    "model = gpflow.models.SVGP(\n",
    "    kernel=graph_kernel,\n",
    "    likelihood=likelihood,\n",
    "    inducing_variable=Z,\n",
    "    num_latent_gps=cls_number,\n",
    "    whiten=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8ebd6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf12474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_STEPS    = 2000\n",
    "CHECK_EVERY  = 100     # run validation every 100 steps\n",
    "\n",
    "\n",
    "# 2.3 Create a tf.data.Dataset for the 112 “inner” training nodes\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_fit, Y_fit)) \\\n",
    "                         .shuffle(len(X_fit), seed=42) \\\n",
    "                         .batch(len(X_fit))   \\\n",
    "                         .repeat()            # <-- add .repeat() here\n",
    "\n",
    "train_iter = iter(train_ds)  # now this iterator never runs out\n",
    "\n",
    "# 2.4 Adam optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# 2.5 Lists to record training ELBO and validation accuracy\n",
    "elbo_history     = []   # will store ELBO at each step (optional)\n",
    "val_acc_history  = []   # will store (step, val_acc) pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Main training loop\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    # Get one batch of 112 nodes (train_ds now repeats forever)\n",
    "    Xb, Yb = next(train_iter)\n",
    "\n",
    "    # Compute the negative ELBO and take a gradient step\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model.training_loss((Xb, Yb))   # negative ELBO\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Record the positive ELBO (just for monitoring)\n",
    "    elbo = -loss.numpy()\n",
    "    elbo_history.append(elbo)\n",
    "\n",
    "    # Every CHECK_EVERY steps, compute validation accuracy\n",
    "    if step % CHECK_EVERY == 0 or step == 1:\n",
    "        # Predict on the 28 validation nodes\n",
    "        val_probs, _   = model.predict_y(X_val)          # tensor shape (28, cls_number)\n",
    "        val_preds      = np.argmax(val_probs.numpy(), axis=1)  # shape (28,)\n",
    "        val_acc        = accuracy_score(Y_val, val_preds)\n",
    "\n",
    "        val_acc_history.append((step, val_acc))\n",
    "        print(f\"Step {step:4d}  | ELBO = {elbo:.4e}  | Val Acc = {val_acc*100:.2f}%\")\n",
    "\n",
    "# 2.7 End of training loop\n",
    "print(\"\\nTraining loop finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8edd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_STEP = 800   # choose from your validation curve\n",
    "\n",
    "# 4.1 Rebuild the PoFM + SVGP model\n",
    "final_kernel = GraphGeneralPoFMKernel(\n",
    "    adjacency_matrix,\n",
    "    max_walk_length=MAX_WALK_LENGTH,\n",
    "    normalize_laplacian=True\n",
    ")\n",
    "final_likelihood = gpflow.likelihoods.MultiClass(num_classes=cls_number)\n",
    "\n",
    "final_model = gpflow.models.SVGP(\n",
    "    kernel=final_kernel,\n",
    "    likelihood=final_likelihood,\n",
    "    inducing_variable=Z,\n",
    "    num_latent_gps=cls_number,\n",
    "    whiten=True,\n",
    ")\n",
    "\n",
    "# 4.2 Create a dataset of all 140 nodes, repeating indefinitely\n",
    "full_train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "                              .shuffle(train_num, seed=42) \\\n",
    "                              .batch(train_num)  \\\n",
    "                              .repeat()         # so next() never stops\n",
    "\n",
    "final_train_iter = iter(full_train_ds)\n",
    "\n",
    "# 4.3 Same optimizer\n",
    "final_optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# 4.4 Training loop for exactly BEST_STEP iterations\n",
    "for step in range(1, BEST_STEP + 1):\n",
    "    Xb, Yb = next(final_train_iter)  # now returns all 140 each time\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = final_model.training_loss((Xb, Yb))\n",
    "    grads = tape.gradient(loss, final_model.trainable_variables)\n",
    "    final_optimizer.apply_gradients(zip(grads, final_model.trainable_variables))\n",
    "\n",
    "# 4.5 Evaluate on the 1000 test nodes\n",
    "probs_test, _ = final_model.predict_y(x_test)\n",
    "y_pred_test   = np.argmax(probs_test.numpy(), axis=1)\n",
    "test_acc      = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Final Test Accuracy (after {BEST_STEP} steps): {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
