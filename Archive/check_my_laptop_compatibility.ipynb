{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac6a549",
   "metadata": {},
   "source": [
    "# M2 MacBook Compatibility Check\n",
    "\n",
    "This notebook verifies that your M2 MacBook is properly configured for running the Efficient Gaussian Process on Graphs project with optimal performance.\n",
    "\n",
    "## What we'll check:\n",
    "1. PyTorch installation and MPS (Metal Performance Shaders) support\n",
    "2. Basic tensor operations on GPU\n",
    "3. Custom graph kernel compatibility\n",
    "4. Memory usage and performance benchmarks\n",
    "5. Required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539bbe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM INFORMATION ===\n",
      "Python version: 3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Platform: macOS-15.5-arm64-arm-64bit\n",
      "Architecture: arm64\n",
      "Processor: arm\n",
      "CPU cores: 8 physical, 8 logical\n",
      "Total RAM: 8.0 GB\n",
      "\n",
      "=== PYTORCH INFORMATION ===\n",
      "PyTorch version: 2.7.1\n",
      "MPS (Metal) available: True\n",
      "MPS built: True\n",
      "‚úÖ MPS is available - your M2 MacBook is ready for GPU acceleration!\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import platform\n",
    "import psutil\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== SYSTEM INFORMATION ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"CPU cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "print(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print()\n",
    "\n",
    "print(\"=== PYTORCH INFORMATION ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check MPS support (Metal Performance Shaders for Apple Silicon)\n",
    "try:\n",
    "    mps_available = torch.backends.mps.is_available()\n",
    "    mps_built = torch.backends.mps.is_built()\n",
    "    print(f\"MPS (Metal) available: {mps_available}\")\n",
    "    print(f\"MPS built: {mps_built}\")\n",
    "except AttributeError:\n",
    "    print(\"MPS support not available in this PyTorch version\")\n",
    "    mps_available = False\n",
    "    mps_built = False\n",
    "\n",
    "# Device selection for Mac\n",
    "if mps_available:\n",
    "    print(\"‚úÖ MPS is available - your M2 MacBook is ready for GPU acceleration!\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  MPS not available - using CPU only\")\n",
    "    print(\"   Consider updating PyTorch: pip install torch torchvision torchaudio\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8186ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING BASIC TENSOR OPERATIONS ===\n",
      "\n",
      "Testing 100x100 matrices...\n",
      "  CPU time: 0.0163s\n",
      "  MPS time: 0.1242s\n",
      "  Speedup: 0.13x\n",
      "\n",
      "Testing 1000x1000 matrices...\n",
      "  CPU time: 0.0229s\n",
      "  MPS time: 0.0805s\n",
      "  Speedup: 0.28x\n",
      "\n",
      "Testing 5000x5000 matrices...\n",
      "  CPU time: 0.0163s\n",
      "  MPS time: 0.1242s\n",
      "  Speedup: 0.13x\n",
      "\n",
      "Testing 1000x1000 matrices...\n",
      "  CPU time: 0.0229s\n",
      "  MPS time: 0.0805s\n",
      "  Speedup: 0.28x\n",
      "\n",
      "Testing 5000x5000 matrices...\n",
      "  CPU time: 0.7267s\n",
      "  MPS time: 0.2606s\n",
      "  Speedup: 2.79x\n",
      "\n",
      "Average MPS speedup: 1.07x\n",
      "‚ö†Ô∏è  Limited MPS acceleration - check PyTorch MPS installation\n",
      "  CPU time: 0.7267s\n",
      "  MPS time: 0.2606s\n",
      "  Speedup: 2.79x\n",
      "\n",
      "Average MPS speedup: 1.07x\n",
      "‚ö†Ô∏è  Limited MPS acceleration - check PyTorch MPS installation\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TESTING BASIC TENSOR OPERATIONS ===\")\n",
    "\n",
    "# Test basic tensor creation and operations\n",
    "sizes = [100, 1000, 5000]\n",
    "results = {}\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"\\nTesting {size}x{size} matrices...\")\n",
    "    \n",
    "    # CPU test\n",
    "    start_time = time.time()\n",
    "    a_cpu = torch.randn(size, size)\n",
    "    b_cpu = torch.randn(size, size)\n",
    "    c_cpu = torch.mm(a_cpu, b_cpu)\n",
    "    cpu_time = time.time() - start_time\n",
    "    \n",
    "    # MPS test (if available)\n",
    "    if device.type == \"mps\":\n",
    "        start_time = time.time()\n",
    "        a_gpu = torch.randn(size, size, device=device)\n",
    "        b_gpu = torch.randn(size, size, device=device)\n",
    "        \n",
    "        # Synchronization for accurate timing\n",
    "        torch.mps.synchronize()\n",
    "            \n",
    "        start_time = time.time()\n",
    "        c_gpu = torch.mm(a_gpu, b_gpu)\n",
    "        \n",
    "        torch.mps.synchronize()\n",
    "            \n",
    "        gpu_time = time.time() - start_time\n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "        \n",
    "        print(f\"  CPU time: {cpu_time:.4f}s\")\n",
    "        print(f\"  MPS time: {gpu_time:.4f}s\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        results[size] = {\"cpu\": cpu_time, \"gpu\": gpu_time, \"speedup\": speedup}\n",
    "    else:\n",
    "        print(f\"  CPU time: {cpu_time:.4f}s\")\n",
    "        results[size] = {\"cpu\": cpu_time}\n",
    "\n",
    "if device.type == \"mps\" and results:\n",
    "    avg_speedup = np.mean([r[\"speedup\"] for r in results.values()])\n",
    "    print(f\"\\nAverage MPS speedup: {avg_speedup:.2f}x\")\n",
    "    if avg_speedup > 2.0:\n",
    "        print(\"‚úÖ Excellent MPS acceleration!\")\n",
    "    elif avg_speedup > 1.2:\n",
    "        print(\"‚úÖ Good MPS acceleration\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Limited MPS acceleration - check PyTorch MPS installation\")\n",
    "elif device.type == \"cpu\":\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU acceleration available - consider installing PyTorch with MPS support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f78c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING PROJECT DEPENDENCIES ===\n",
      "‚úÖ torch\n",
      "‚úÖ gpytorch\n",
      "‚úÖ numpy\n",
      "‚úÖ scipy‚úÖ scipy\n",
      "‚úÖ matplotlib\n",
      "\n",
      "‚úÖ matplotlib\n",
      "‚úÖ networkx\n",
      "‚ùå scikit-learn - MISSING\n",
      "‚úÖ psutil\n",
      "\n",
      "‚ö†Ô∏è  Missing packages: scikit-learn\n",
      "Install with: pip install scikit-learn\n",
      "‚úÖ networkx\n",
      "‚ùå scikit-learn - MISSING\n",
      "‚úÖ psutil\n",
      "\n",
      "‚ö†Ô∏è  Missing packages: scikit-learn\n",
      "Install with: pip install scikit-learn\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CHECKING PROJECT DEPENDENCIES ===\")\n",
    "\n",
    "required_packages = [\n",
    "    \"torch\", \"gpytorch\", \"numpy\", \"scipy\", \"matplotlib\", \n",
    "    \"networkx\", \"scikit-learn\", \"psutil\"\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package} - MISSING\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"Install with: pip install \" + \" \".join(missing_packages))\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f7df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING CUSTOM GRAPH KERNEL ===\n",
      "‚úÖ Project path found: /Users/matthew/Documents/Efficient Gaussian Process on Graphs/Efficient_Gaussian_Process_On_Graphs\n",
      "‚úÖ Successfully imported GraphGeneralFastGRFKernel\n",
      "\n",
      "Testing kernel creation...\n",
      "Kernel initialized with 3 step matrices\n",
      "Using device-aware dense tensors for MPS compatibility\n",
      "‚úÖ Kernel created successfully\n",
      "Testing kernel evaluation...\n",
      "‚úÖ Kernel evaluation successful - shape: torch.Size([3, 2])\n",
      "‚ùå Error testing kernel: 'LazyEvaluatedKernelTensor' object has no attribute 'min'\n",
      "‚úÖ Successfully imported GraphGeneralFastGRFKernel\n",
      "\n",
      "Testing kernel creation...\n",
      "Kernel initialized with 3 step matrices\n",
      "Using device-aware dense tensors for MPS compatibility\n",
      "‚úÖ Kernel created successfully\n",
      "Testing kernel evaluation...\n",
      "‚úÖ Kernel evaluation successful - shape: torch.Size([3, 2])\n",
      "‚ùå Error testing kernel: 'LazyEvaluatedKernelTensor' object has no attribute 'min'\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TESTING CUSTOM GRAPH KERNEL ===\")\n",
    "\n",
    "# Test importing the custom kernel\n",
    "try:\n",
    "    project_path = Path(\"/Users/matthew/Documents/Efficient Gaussian Process on Graphs/Efficient_Gaussian_Process_On_Graphs\")\n",
    "    if not project_path.exists():\n",
    "        print(\"‚ùå Project path not found!\")\n",
    "        print(f\"Expected: {project_path}\")\n",
    "        print(\"Please check the project location.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Project path found: {project_path}\")\n",
    "        \n",
    "    # Add to path\n",
    "    sys.path.append(str(project_path))\n",
    "    sys.path.append(str(project_path / \"efficient_graph_gp_sparse\"))\n",
    "    \n",
    "    # Try importing the kernel\n",
    "    from efficient_graph_gp_sparse.gptorch_kernels_sparse.general_kernel_fast_grf import GraphGeneralFastGRFKernel\n",
    "    print(\"‚úÖ Successfully imported GraphGeneralFastGRFKernel\")\n",
    "    \n",
    "    # Test basic kernel creation\n",
    "    import scipy.sparse as sp\n",
    "    import networkx as nx\n",
    "    \n",
    "    # Create a small test graph\n",
    "    G = nx.cycle_graph(10)\n",
    "    A = nx.adjacency_matrix(G).tocsr()\n",
    "    \n",
    "    print(\"\\nTesting kernel creation...\")\n",
    "    kernel = GraphGeneralFastGRFKernel(\n",
    "        adjacency_matrix=A,\n",
    "        walks_per_node=1000,\n",
    "        p_halt=0.1,\n",
    "        max_walk_length=3,\n",
    "        random_walk_seed=42\n",
    "    )\n",
    "    print(\"‚úÖ Kernel created successfully\")\n",
    "    \n",
    "    # Test kernel evaluation\n",
    "    x1 = torch.tensor([[0], [1], [2]], dtype=torch.float32, device=device)\n",
    "    x2 = torch.tensor([[0], [1]], dtype=torch.float32, device=device)\n",
    "    \n",
    "    print(\"Testing kernel evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        K = kernel(x1, x2)\n",
    "        print(f\"‚úÖ Kernel evaluation successful - shape: {K.shape}\")\n",
    "        print(f\"Kernel values range: [{K.min().item():.4f}, {K.max().item():.4f}]\")\n",
    "        \n",
    "    # Check if using MPS optimization\n",
    "    if hasattr(kernel, 'step_matrices_dense') and device.type == \"mps\":\n",
    "        print(\"‚úÖ Using MPS-optimized dense implementation\")\n",
    "        print(f\"Step matrices on device: {kernel.step_matrices_dense.device}\")\n",
    "    elif hasattr(kernel, 'step_matrices'):\n",
    "        print(\"‚úÖ Using sparse implementation\")\n",
    "        print(f\"Number of step matrices: {len(kernel.step_matrices)}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import custom kernel: {e}\")\n",
    "    print(\"Check that the project structure is correct and all files are present.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing kernel: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd98f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMORY AND PERFORMANCE TEST ===\n",
      "Current memory usage: 86.0%\n",
      "Available memory: 1.1 GB\n",
      "\n",
      "Testing with 100 node graph...\n",
      "Graph edges: 291\n",
      "Adjacency sparsity: 0.0582\n",
      "Kernel initialized with 3 step matrices\n",
      "Using device-aware dense tensors for MPS compatibility\n",
      "Kernel initialized with 3 step matrices\n",
      "Using device-aware dense tensors for MPS compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Documents/Efficient Gaussian Process on Graphs/Efficient_Gaussian_Process_On_Graphs/venv/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forward pass completed in 0.1903s\n",
      "‚úÖ Prediction completed - mean: [-0.39313266  0.10191169]\n",
      "Additional memory used: -6.0%\n",
      "‚úÖ Excellent memory efficiency!\n",
      "‚úÖ Prediction completed - mean: [-0.39313266  0.10191169]\n",
      "Additional memory used: -6.0%\n",
      "‚úÖ Excellent memory efficiency!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MEMORY AND PERFORMANCE TEST ===\")\n",
    "\n",
    "# Check current memory usage\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Current memory usage: {memory_info.percent:.1f}%\")\n",
    "print(f\"Available memory: {memory_info.available / (1024**3):.1f} GB\")\n",
    "\n",
    "# Test with larger graph if kernel import was successful\n",
    "try:\n",
    "    from efficient_graph_gp_sparse.gptorch_kernels_sparse.general_kernel_fast_grf import GraphGeneralFastGRFKernel\n",
    "    import gpytorch\n",
    "    \n",
    "    # Create larger test graph\n",
    "    n_nodes = 100\n",
    "    G = nx.barabasi_albert_graph(n_nodes, 3, seed=42)\n",
    "    A = nx.adjacency_matrix(G).tocsr()\n",
    "    \n",
    "    print(f\"\\nTesting with {n_nodes} node graph...\")\n",
    "    print(f\"Graph edges: {G.number_of_edges()}\")\n",
    "    print(f\"Adjacency sparsity: {A.nnz / (n_nodes * n_nodes):.4f}\")\n",
    "    \n",
    "    # Create kernel and model\n",
    "    kernel = GraphGeneralFastGRFKernel(\n",
    "        adjacency_matrix=A,\n",
    "        walks_per_node=5000,\n",
    "        p_halt=0.1,\n",
    "        max_walk_length=3,\n",
    "        random_walk_seed=42\n",
    "    )\n",
    "    \n",
    "    # Test GP model creation\n",
    "    n_train = 50\n",
    "    train_idx = np.random.choice(n_nodes, size=n_train, replace=False)\n",
    "    X_train = torch.tensor(train_idx, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train = torch.randn(n_train, device=device)\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    \n",
    "    class TestGPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "            super().__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(kernel)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    model = TestGPModel(X_train, y_train, likelihood, kernel).to(device)\n",
    "    \n",
    "    # Test forward pass timing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        output = model(X_train)\n",
    "        forward_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Forward pass completed in {forward_time:.4f}s\")\n",
    "        \n",
    "        # Test prediction on new points\n",
    "        X_test = torch.tensor([[25], [75]], dtype=torch.float32, device=device)\n",
    "        pred = likelihood(model(X_test))\n",
    "        print(f\"‚úÖ Prediction completed - mean: {pred.mean.cpu().numpy()}\")\n",
    "    \n",
    "    # Check memory usage after operations\n",
    "    memory_after = psutil.virtual_memory()\n",
    "    memory_used = memory_after.percent - memory_info.percent\n",
    "    print(f\"Additional memory used: {memory_used:.1f}%\")\n",
    "    \n",
    "    if memory_used < 5.0:\n",
    "        print(\"‚úÖ Excellent memory efficiency!\")\n",
    "    elif memory_used < 15.0:\n",
    "        print(\"‚úÖ Good memory usage\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  High memory usage - consider smaller graphs or batch processing\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Performance test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12729351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OPTIMIZATION RECOMMENDATIONS ===\n",
      "Current PyTorch settings:\n",
      "  torch.get_num_threads(): 4\n",
      "  MKL-DNN enabled: False\n",
      "\n",
      "==================================================\n",
      "RECOMMENDATIONS:\n",
      "‚Ä¢ Consider setting torch.set_num_threads(8) for optimal CPU performance\n",
      "‚Ä¢ Low available memory - consider closing other applications\n",
      "‚Ä¢ Using MPS device - optimal for M2 MacBook! üöÄ\n",
      "‚Ä¢ For large graphs (>1000 nodes), consider batch processing\n",
      "‚Ä¢ MPS works best with float32 tensors\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=== OPTIMIZATION RECOMMENDATIONS ===\")\n",
    "\n",
    "# Check PyTorch settings\n",
    "print(\"Current PyTorch settings:\")\n",
    "print(f\"  torch.get_num_threads(): {torch.get_num_threads()}\")\n",
    "try:\n",
    "    print(f\"  MKL-DNN enabled: {torch.backends.mkldnn.is_available()}\")\n",
    "except:\n",
    "    print(\"  MKL-DNN: Not available\")\n",
    "\n",
    "# Recommendations based on results\n",
    "recommendations = []\n",
    "\n",
    "if device.type == \"cpu\":\n",
    "    recommendations.append(\"‚Ä¢ Install PyTorch with MPS support for M2 acceleration:\")\n",
    "    recommendations.append(\"  pip install torch torchvision torchaudio\")\n",
    "\n",
    "if torch.get_num_threads() != psutil.cpu_count(logical=False):\n",
    "    recommendations.append(f\"‚Ä¢ Consider setting torch.set_num_threads({psutil.cpu_count(logical=False)}) for optimal CPU performance\")\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "if memory_info.available < 4 * (1024**3):  # Less than 4GB available\n",
    "    recommendations.append(\"‚Ä¢ Low available memory - consider closing other applications\")\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    recommendations.append(\"‚Ä¢ Using MPS device - optimal for M2 MacBook! üöÄ\")\n",
    "    recommendations.append(\"‚Ä¢ For large graphs (>1000 nodes), consider batch processing\")\n",
    "    recommendations.append(\"‚Ä¢ MPS works best with float32 tensors\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"‚Ä¢ Your setup looks optimal! üéâ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50047ab1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This compatibility check verifies:\n",
    "\n",
    "‚úÖ **System Requirements**: Python, platform, and hardware details  \n",
    "‚úÖ **PyTorch Installation**: Version, CUDA/MPS support, and device selection  \n",
    "‚úÖ **GPU Acceleration**: Tensor operations benchmarking and speedup measurement  \n",
    "‚úÖ **Dependencies**: All required packages for the project  \n",
    "‚úÖ **Custom Kernels**: Import and basic functionality testing  \n",
    "‚úÖ **Performance**: Memory usage and execution timing  \n",
    "‚úÖ **Optimization**: Settings and recommendations for best performance  \n",
    "\n",
    "If all checks pass with ‚úÖ, your M2 MacBook is ready to run the Efficient Gaussian Process on Graphs project with optimal performance!\n",
    "\n",
    "### Next Steps:\n",
    "1. If any issues were found, follow the recommendations above\n",
    "2. Run the main experiment notebooks in `/experiments_sparse/`\n",
    "3. For large-scale experiments, monitor memory usage and consider batch processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
