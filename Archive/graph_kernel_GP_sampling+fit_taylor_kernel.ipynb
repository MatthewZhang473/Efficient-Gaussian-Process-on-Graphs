{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The functionalities in this notebook is as follows:\n",
    "\n",
    "1. Given a graph, calculate the kernel matrix - diffusion for example\n",
    "\n",
    "2. Sample from a GP with this kernel matrix, for a given hyper parameters\n",
    "\n",
    "3. fit another GP with the same kernel  (and unknown hyperparamters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sample data from a GP on graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor Expansion of the Diffusion Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_laplacian_matrix(W):\n",
    "    \"To get the graph Laplacian from adj matrix, symmetric adjacency matrix is assumed.\"\n",
    "    nb_vertices = len(W)\n",
    "    L = 0.5 * np.eye(nb_vertices)\n",
    "    degrees = np.sum(W,axis=1)\n",
    "    for i in range(nb_vertices):\n",
    "        for j in range(i):\n",
    "            L[i,j] = - W[i,j]/np.sqrt(degrees[i] * degrees[j])\n",
    "    L += L.T\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d9bdd3c87b4c989c90ce39e5c4b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='Beta (σ):', max=10.0, min=0.1), IntSlider(value=11, …"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "from ipywidgets import FloatSlider, IntSlider, interactive\n",
    "\n",
    "def get_laplacian_matrix(W):\n",
    "    \"To get the graph Laplacian from adj matrix, symmetric adjacency matrix is assumed.\"\n",
    "    nb_vertices = len(W)\n",
    "    L = 0.5 * np.eye(nb_vertices)\n",
    "    degrees = np.sum(W,axis=1)\n",
    "    for i in range(nb_vertices):\n",
    "        for j in range(i):\n",
    "            L[i,j] = - W[i,j]/np.sqrt(degrees[i] * degrees[j])\n",
    "    L += L.T\n",
    "    return L\n",
    "\n",
    "def diffusion_kernel(adj_matrix, beta):\n",
    "    \"\"\"Compute the diffusion kernel using matrix exponential.\"\"\"\n",
    "    laplacian = get_laplacian_matrix(adj_matrix)\n",
    "    return expm(-beta * laplacian)\n",
    "\n",
    "def taylor_expansion_diffusion_kernel(adj_matrix, beta, num_terms=11):\n",
    "    \"\"\"Compute the Taylor expansion of the diffusion kernel.\"\"\"\n",
    "    laplacian = get_laplacian_matrix(adj_matrix)\n",
    "    K = np.eye(adj_matrix.shape[0])  # Initialize kernel matrix\n",
    "    for i in range(1, num_terms):\n",
    "        K += (-beta)**i * np.linalg.matrix_power(laplacian, i) / math.factorial(i)\n",
    "\n",
    "    # Check for positive definiteness by ensuring diagonal dominance\n",
    "    min_diagonal = np.min(np.diag(K))\n",
    "    if min_diagonal <= 0:\n",
    "        adjustment = -min_diagonal + 1e-6  # Adding a small epsilon for strict positivity\n",
    "        K += adjustment * np.eye(adj_matrix.shape[0])\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def sample_from_gp(kernel):\n",
    "    \"\"\"Sample from a Gaussian process given a kernel matrix.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    L = np.linalg.cholesky(kernel + 1e-6 * np.eye(kernel.shape[0]))\n",
    "    rv_matrix = np.random.normal(size=(kernel.shape[0], 1))\n",
    "    return L @ rv_matrix\n",
    "\n",
    "def plot_samples_and_kernels(beta, num_terms, num_nodes=20):\n",
    "    \"\"\"Plot sampled values and kernel matrices.\"\"\"\n",
    "    adjacency_matrix = np.eye(num_nodes, k=1) + np.eye(num_nodes, k=-1)  # Circular adjacency matrix\n",
    "    K = diffusion_kernel(adjacency_matrix, beta)\n",
    "    K_taylor = taylor_expansion_diffusion_kernel(adjacency_matrix, beta, num_terms)\n",
    "    \n",
    "    samples = sample_from_gp(K)\n",
    "    samples_taylor = sample_from_gp(K_taylor)\n",
    "    \n",
    "    # Calculate the Frobenius norm of the difference\n",
    "    frobenius_norm = np.linalg.norm(K - K_taylor, 'fro')\n",
    "\n",
    "    # Create a figure with 2x2 subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    def plot_sampled_values(ax, samples, title, color):\n",
    "        ax.plot(range(num_nodes), samples.flatten(), marker='o', linestyle='-', color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Node Number')\n",
    "        ax.set_ylabel('Sampled Value')\n",
    "        ax.set_xticks(range(num_nodes))\n",
    "        ax.grid()\n",
    "\n",
    "    def plot_heatmap(ax, matrix, title):\n",
    "        cax = ax.matshow(matrix, cmap='viridis')\n",
    "        ax.set_title(title)\n",
    "        plt.colorbar(cax, ax=ax)\n",
    "        ax.set_xlabel('Node Index')\n",
    "        ax.set_ylabel('Node Index')\n",
    "\n",
    "    # Plot sampled values and heatmaps\n",
    "    plot_sampled_values(axs[0, 0], samples, f'Sampled Values (beta={beta:.2f})', 'b')\n",
    "    plot_heatmap(axs[0, 1], K, f'Kernel Matrix Heatmap (beta={beta:.2f})')\n",
    "    plot_sampled_values(axs[1, 0], samples_taylor, f'Sampled Values for Taylor Expansion (beta={beta:.2f})', 'r')\n",
    "    plot_heatmap(axs[1, 1], K_taylor, f'Taylor Expansion Heatmap (beta={beta:.2f})\\nFrobenius Norm: {frobenius_norm:.4f}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive sliders for beta and num_terms\n",
    "beta_slider = FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1, description='Beta (σ):')\n",
    "num_terms_slider = IntSlider(value=11, min=1, max=20, step=1, description='Num Terms:')\n",
    "interactive_plot = interactive(plot_samples_and_kernels, beta=beta_slider, num_terms=num_terms_slider)\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the (linear / random) graph we fit and the fitted GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d65ec79dc6049c5b24d4415a638f66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=1.0, description='Beta:', max=2.0, min=0.05, step=0.05),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7485cb0c024cee910e878da36b38f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import networkx as nx\n",
    "\n",
    "def get_laplacian_matrix(adj_matrix):\n",
    "    \"\"\"Compute the Laplacian matrix for the given adjacency matrix.\"\"\"\n",
    "    D = np.diag(np.sum(adj_matrix, axis=1))  # Degree matrix\n",
    "    return D - adj_matrix  # Laplacian matrix\n",
    "\n",
    "def diffusion_kernel(adj_matrix, beta):\n",
    "    \"\"\"Compute the diffusion kernel using matrix exponential.\"\"\"\n",
    "    laplacian = get_laplacian_matrix(adj_matrix)\n",
    "    return expm(-beta * laplacian)\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 100\n",
    "# Generate an undirected random graph\n",
    "probability = 0.03  # Probability of edge creation\n",
    "G = nx.erdos_renyi_graph(num_nodes, probability, directed=False)  # Ensure the graph is undirected\n",
    "adjacency_matrix = nx.to_numpy_array(G)  # Convert to adjacency matrix\n",
    "\n",
    "# Generate noisy samples function\n",
    "def generate_noisy_samples(beta_sample):\n",
    "    K = diffusion_kernel(adjacency_matrix, beta_sample)\n",
    "    L = np.linalg.cholesky(K + 1e-6 * np.eye(num_nodes))  # Cholesky decomposition\n",
    "    true_samples = L @ np.random.normal(size=(num_nodes, 1))  # Sample from Gaussian process\n",
    "    noise = 0.1 * np.random.randn(num_nodes, 1)  # Additive noise\n",
    "    Y_noisy = true_samples + noise  # Noisy observations\n",
    "    return Y_noisy\n",
    "\n",
    "# Modified `K` function in GraphDiffusionKernel class\n",
    "class GraphDiffusionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, adjacency_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.adjacency_matrix = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float64)\n",
    "        self.beta = gpflow.Parameter(2.0, transform=gpflow.utilities.positive())  # Learnable hyperparameter\n",
    "\n",
    "    def K(self, X1, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X1\n",
    "        # Compute the full diffusion kernel\n",
    "        kernel_matrix = self.diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X1 = tf.cast(tf.reshape(X1, [-1]), dtype=tf.int32)\n",
    "        indices_X2 = tf.cast(tf.reshape(X2, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.gather(kernel_matrix, indices_X1, axis=0), indices_X2, axis=1)\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        kernel_matrix = self.diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X = tf.cast(tf.reshape(X, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.linalg.diag_part(kernel_matrix), indices_X)\n",
    "    \n",
    "    def get_laplacian_matrix(self, W):\n",
    "        \"\"\"Get the graph Laplacian from the adjacency matrix.\"\"\"\n",
    "        nb_vertices = len(W)\n",
    "        L = 0.5 * np.eye(nb_vertices)\n",
    "        degrees = np.sum(W, axis=1)\n",
    "        for i in range(nb_vertices):\n",
    "            for j in range(i):\n",
    "                L[i, j] = -W[i, j] / np.sqrt(degrees[i] * degrees[j])\n",
    "        L += L.T\n",
    "        return L\n",
    "\n",
    "    def diffusion_kernel(self, adj_matrix, beta):\n",
    "        laplacian = get_laplacian_matrix(adj_matrix)  # Graph Laplacian\n",
    "        return tf.linalg.expm(-beta * laplacian)\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_results(beta_sample):\n",
    "    clear_output(wait=True)  # Clear previous output\n",
    "    # Generate noisy samples\n",
    "    Y_noisy = generate_noisy_samples(beta_sample)\n",
    "\n",
    "    # Convert noisy data to TensorFlow tensor\n",
    "    X = tf.convert_to_tensor(np.arange(num_nodes, dtype=np.float64).reshape(-1, 1))  # Input features (nodes)\n",
    "    Y = tf.convert_to_tensor(Y_noisy, dtype=tf.float64)  # Noisy sampled data\n",
    "\n",
    "    # Create an instance of the kernel\n",
    "    graph_kernel = GraphDiffusionKernel(adjacency_matrix)\n",
    "\n",
    "    # GPflow model\n",
    "    model = gpflow.models.GPR(data=(X, Y), kernel=graph_kernel, mean_function=None)\n",
    "\n",
    "    # Optimize the model\n",
    "    gpflow.optimizers.Scipy().minimize(model.training_loss, model.trainable_variables)\n",
    "\n",
    "    # Prediction for visualization\n",
    "    X_new = tf.convert_to_tensor(np.arange(num_nodes).reshape(-1, 1), dtype=tf.float64)  # New input features for prediction\n",
    "    mean, variance = model.predict_f(X_new)  # Predict mean and variance\n",
    "    stddev = tf.sqrt(variance)  # Standard deviation\n",
    "\n",
    "    # Create side-by-side plots for GP and network graph\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Gaussian Process plot on the left\n",
    "    ax[0].plot(X.numpy(), Y.numpy(), 'ro', label='Noisy Samples')\n",
    "    ax[0].plot(X_new.numpy(), mean.numpy(), 'b-', label='Fitted Mean')\n",
    "    ax[0].fill_between(X_new.numpy().flatten(),\n",
    "                       (mean - 1.96 * stddev).numpy().flatten(),\n",
    "                       (mean + 1.96 * stddev).numpy().flatten(),\n",
    "                       color='lightblue', alpha=0.5, label='95% Confidence Interval')\n",
    "    ax[0].set_title(f'Gaussian Process Fit with Graph Diffusion Kernel (beta={beta_sample:.2f})')\n",
    "    ax[0].set_xlabel('Node Number')\n",
    "    ax[0].set_ylabel('Sampled Value')\n",
    "    ax[0].grid()\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # Network graph plot on the right\n",
    "    plot_network_graph(adjacency_matrix, ax[1])\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Learned beta:\", model.kernel.beta.numpy())\n",
    "\n",
    "# Function to plot the network graph in Matplotlib\n",
    "def plot_network_graph(adjacency_matrix, ax):\n",
    "    # Create a NetworkX graph from the adjacency matrix\n",
    "    G = nx.from_numpy_array(adjacency_matrix)\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=300, node_color='red')\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='blue')\n",
    "    \n",
    "    ax.set_title(\"Network Graph Representation of Adjacency Matrix\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Create a slider for beta\n",
    "beta_slider = widgets.FloatSlider(value=1.0, min=0.05, max=2.0, step=0.05, description='Beta:')\n",
    "ui = widgets.VBox([beta_slider])\n",
    "\n",
    "# Link the slider to the plotting function\n",
    "out = widgets.interactive_output(plot_results, {'beta_sample': beta_slider})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fb7eed15c54027a76cc08d4514ebc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=0.3, description='Beta:', max=2.0, min=0.05, step=0.05),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801957f6e0e4af8814281649f172800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import plotly.graph_objs as go\n",
    "import networkx as nx\n",
    "\n",
    "def get_laplacian_matrix(W):\n",
    "    \"To get the graph Laplacian from adj matrix, symmetric adjacency matrix is assumed.\"\n",
    "    nb_vertices = len(W)\n",
    "    L = 0.5 * np.eye(nb_vertices)\n",
    "    degrees = np.sum(W,axis=1)\n",
    "    for i in range(nb_vertices):\n",
    "        for j in range(i):\n",
    "            L[i,j] = - W[i,j]/np.sqrt(degrees[i] * degrees[j])\n",
    "    L += L.T\n",
    "    return L\n",
    "\n",
    "# Function to compute the diffusion kernel\n",
    "def diffusion_kernel(adj_matrix, beta):\n",
    "    laplacian = get_laplacian_matrix(adj_matrix)  # Graph Laplacian\n",
    "    return expm(-beta * laplacian)  # Matrix exponential\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 100\n",
    "adjacency_matrix = np.eye(num_nodes, k=1) + np.eye(num_nodes, k=-1)  # Circular adjacency matrix\n",
    "\n",
    "# Generate noisy samples function\n",
    "def generate_noisy_samples(beta_sample):\n",
    "    K = diffusion_kernel(adjacency_matrix, beta_sample)\n",
    "    L = np.linalg.cholesky(K + 1e-6 * np.eye(num_nodes))  # Cholesky decomposition\n",
    "    true_samples = L @ np.random.normal(size=(num_nodes, 1))  # Sample from Gaussian process\n",
    "    noise = 0.05 * np.random.randn(num_nodes, 1)  # Additive noise\n",
    "    Y_noisy = true_samples + noise  # Noisy observations\n",
    "    return Y_noisy\n",
    "\n",
    "class TaylorExpansionDiffusionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, adjacency_matrix, num_terms=15, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.adjacency_matrix = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float64)\n",
    "        self.beta = gpflow.Parameter(2.0, transform=gpflow.utilities.positive())  # Learnable hyperparameter\n",
    "        self.num_terms = num_terms  # Number of terms for Taylor expansion\n",
    "\n",
    "    def K(self, X1, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X1\n",
    "        # Compute the Taylor expansion kernel matrix\n",
    "        kernel_matrix = self.taylor_expansion_diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X1 = tf.cast(tf.reshape(X1, [-1]), dtype=tf.int32)\n",
    "        indices_X2 = tf.cast(tf.reshape(X2, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.gather(kernel_matrix, indices_X1, axis=0), indices_X2, axis=1)\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        kernel_matrix = self.taylor_expansion_diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X = tf.cast(tf.reshape(X, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.linalg.diag_part(kernel_matrix), indices_X)\n",
    "\n",
    "    def get_laplacian_matrix(self, W):\n",
    "        \"\"\"Get the graph Laplacian from the adjacency matrix.\"\"\"\n",
    "        W = np.array(W)  # Ensure W is a numpy array\n",
    "        nb_vertices = len(W)\n",
    "        L = np.eye(nb_vertices) * np.sum(W, axis=1)\n",
    "        for i in range(nb_vertices):\n",
    "            for j in range(i):\n",
    "                if W[i, j] > 0:\n",
    "                    L[i, j] = L[j, i] = -W[i, j] / np.sqrt(np.sum(W[i]) * np.sum(W[j]))\n",
    "        return L\n",
    "\n",
    "    def taylor_expansion_diffusion_kernel(self, adj_matrix, beta):\n",
    "        \"\"\"Compute the Taylor expansion of the diffusion kernel.\"\"\"\n",
    "        laplacian = self.get_laplacian_matrix(adj_matrix)\n",
    "        K = np.eye(adj_matrix.shape[0])  # Initialize kernel matrix\n",
    "        for i in range(1, self.num_terms):\n",
    "            K += (-beta)**i * np.linalg.matrix_power(laplacian, i) / math.factorial(i)\n",
    "        \n",
    "        # K += 1e-2 * np.eye(adj_matrix.shape[0]) # Adding a small epsilon for numerical stability\n",
    "\n",
    "        return tf.convert_to_tensor(K, dtype=tf.float64)  # Convert to TensorFlow tensor\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_results(beta_sample):\n",
    "    clear_output(wait=True)  # Clear previous output\n",
    "    # Generate noisy samples\n",
    "    Y_noisy = generate_noisy_samples(beta_sample)\n",
    "\n",
    "    # Convert noisy data to TensorFlow tensor\n",
    "    X = tf.convert_to_tensor(np.arange(num_nodes, dtype=np.float64).reshape(-1, 1))  # Input features (nodes)\n",
    "    Y = tf.convert_to_tensor(Y_noisy, dtype=tf.float64)  # Noisy sampled data\n",
    "\n",
    "    # Create an instance of the kernel\n",
    "    graph_kernel = TaylorExpansionDiffusionKernel(adjacency_matrix)\n",
    "\n",
    "    # GPflow model\n",
    "    model = gpflow.models.GPR(data=(X, Y), kernel=graph_kernel, mean_function=None)\n",
    "\n",
    "    # Optimize the model\n",
    "    gpflow.optimizers.Scipy().minimize(model.training_loss, model.trainable_variables)\n",
    "\n",
    "    # Prediction for visualization\n",
    "    X_new = tf.convert_to_tensor(np.arange(num_nodes).reshape(-1, 1), dtype=tf.float64)  # New input features for prediction\n",
    "    mean, variance = model.predict_f(X_new)  # Predict mean and variance\n",
    "    print('Average Variance: ',variance.numpy().mean())\n",
    "    stddev = tf.sqrt(variance)  # Standard deviation\n",
    "\n",
    "    # Plotting the Gaussian Process results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # GP plot on the left\n",
    "    ax[0].plot(X.numpy(), Y.numpy(), 'ro', label='Noisy Samples')\n",
    "    ax[0].plot(X_new.numpy(), mean.numpy(), 'b-', label='Fitted Mean')\n",
    "    ax[0].fill_between(X_new.numpy().flatten(),\n",
    "                       (mean - 1.96 * stddev).numpy().flatten(),\n",
    "                       (mean + 1.96 * stddev).numpy().flatten(),\n",
    "                       color='lightblue', alpha=0.5, label='95% Confidence Interval')\n",
    "    ax[0].set_title(f'Gaussian Process Fit with Graph Diffusion Kernel (beta={beta_sample})')\n",
    "    ax[0].set_xlabel('Node Number')\n",
    "    ax[0].set_ylabel('Sampled Value')\n",
    "    ax[0].grid()\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # Plot the network graph using NetworkX and Plotly\n",
    "    plot_network_graph(adjacency_matrix, ax[1])\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Learned beta:\", model.kernel.beta.numpy())\n",
    "\n",
    "# Function to plot the network graph in Matplotlib\n",
    "def plot_network_graph(adjacency_matrix, ax):\n",
    "    # Create a NetworkX graph from the adjacency matrix\n",
    "    G = nx.from_numpy_array(adjacency_matrix)\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=300, node_color='red')\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='blue')\n",
    "    \n",
    "    ax.set_title(\"Network Graph Representation of Adjacency Matrix\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Create a slider for beta\n",
    "beta_slider = widgets.FloatSlider(value=0.3, min=0.05, max=2.0, step=0.05, description='Beta:')\n",
    "ui = widgets.VBox([beta_slider])\n",
    "\n",
    "# Link the slider to the plotting function\n",
    "out = widgets.interactive_output(plot_results, {'beta_sample': beta_slider})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6827316515b54484a5fe6e91e684841a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=0.2, description='Beta:', max=2.0, min=0.05, step=0.05),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa82ee9e75ea4b79a898a4b8518bb747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import networkx as nx\n",
    "\n",
    "def get_laplacian_matrix(W):\n",
    "    \"To get the graph Laplacian from adj matrix, symmetric adjacency matrix is assumed.\"\n",
    "    nb_vertices = len(W)\n",
    "    L = 0.5 * np.eye(nb_vertices)\n",
    "    degrees = np.sum(W,axis=1)\n",
    "    for i in range(nb_vertices):\n",
    "        for j in range(i):\n",
    "            L[i,j] = - W[i,j]/np.sqrt(degrees[i] * degrees[j])\n",
    "    L += L.T\n",
    "    return L\n",
    "\n",
    "def diffusion_kernel(adj_matrix, beta):\n",
    "    \"\"\"Compute the diffusion kernel using matrix exponential.\"\"\"\n",
    "    laplacian = get_laplacian_matrix(adj_matrix)\n",
    "    return expm(-beta * laplacian)\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 50\n",
    "# Generate an undirected random graph\n",
    "probability = 0.1  # Probability of edge creation\n",
    "G = nx.erdos_renyi_graph(num_nodes, probability, directed=False)  # Ensure the graph is undirected\n",
    "adjacency_matrix = nx.to_numpy_array(G)  # Convert to adjacency matrix\n",
    "\n",
    "# Generate noisy samples function\n",
    "def generate_noisy_samples(beta_sample):\n",
    "    K = diffusion_kernel(adjacency_matrix, beta_sample)\n",
    "    L = np.linalg.cholesky(K + 1e-6 * np.eye(num_nodes))  # Cholesky decomposition\n",
    "    true_samples = L @ np.random.normal(size=(num_nodes, 1))  # Sample from Gaussian process\n",
    "    noise = 0.05 * np.random.randn(num_nodes, 1)  # Additive noise\n",
    "    Y_noisy = true_samples + noise  # Noisy observations\n",
    "    return Y_noisy\n",
    "\n",
    "class TaylorExpansionDiffusionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, adjacency_matrix, num_terms=21, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.adjacency_matrix = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float64)\n",
    "        self.beta = gpflow.Parameter(2.0, transform=gpflow.utilities.positive())  # Learnable hyperparameter\n",
    "        self.num_terms = num_terms  # Number of terms for Taylor expansion\n",
    "\n",
    "    def K(self, X1, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X1\n",
    "        # Compute the Taylor expansion kernel matrix\n",
    "        kernel_matrix = self.taylor_expansion_diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X1 = tf.cast(tf.reshape(X1, [-1]), dtype=tf.int32)\n",
    "        indices_X2 = tf.cast(tf.reshape(X2, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.gather(kernel_matrix, indices_X1, axis=0), indices_X2, axis=1)\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        kernel_matrix = self.taylor_expansion_diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X = tf.cast(tf.reshape(X, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.linalg.diag_part(kernel_matrix), indices_X)\n",
    "\n",
    "    def get_laplacian_matrix(self, W):\n",
    "        \"\"\"Get the graph Laplacian from the adjacency matrix.\"\"\"\n",
    "        W = np.array(W)  # Ensure W is a numpy array\n",
    "        nb_vertices = len(W)\n",
    "        L = np.eye(nb_vertices) * np.sum(W, axis=1)\n",
    "        for i in range(nb_vertices):\n",
    "            for j in range(i):\n",
    "                if W[i, j] > 0:\n",
    "                    L[i, j] = L[j, i] = -W[i, j] / np.sqrt(np.sum(W[i]) * np.sum(W[j]))\n",
    "        return L\n",
    "\n",
    "    def taylor_expansion_diffusion_kernel(self, adj_matrix, beta):\n",
    "        \"\"\"Compute the Taylor expansion of the diffusion kernel.\"\"\"\n",
    "        laplacian = self.get_laplacian_matrix(adj_matrix)\n",
    "        K = np.eye(adj_matrix.shape[0])  # Initialize kernel matrix\n",
    "        for i in range(1, self.num_terms):\n",
    "            K += (-beta)**i * np.linalg.matrix_power(laplacian, i) / math.factorial(i)\n",
    "        \n",
    "        # K += 1e-2 * np.eye(adj_matrix.shape[0]) # Adding a small epsilon for numerical stability\n",
    "\n",
    "        return tf.convert_to_tensor(K, dtype=tf.float64)  # Convert to TensorFlow tensor\n",
    "\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_results(beta_sample):\n",
    "    clear_output(wait=True)  # Clear previous output\n",
    "    # Generate noisy samples\n",
    "    Y_noisy = generate_noisy_samples(beta_sample)\n",
    "\n",
    "    # Convert noisy data to TensorFlow tensor\n",
    "    X = tf.convert_to_tensor(np.arange(num_nodes, dtype=np.float64).reshape(-1, 1))  # Input features (nodes)\n",
    "    Y = tf.convert_to_tensor(Y_noisy, dtype=tf.float64)  # Noisy sampled data\n",
    "\n",
    "    # Create an instance of the kernel\n",
    "    graph_kernel = TaylorExpansionDiffusionKernel(adjacency_matrix)\n",
    "\n",
    "    # GPflow model\n",
    "    model = gpflow.models.GPR(data=(X, Y), kernel=graph_kernel, mean_function=None)\n",
    "\n",
    "    # Optimize the model\n",
    "    gpflow.optimizers.Scipy().minimize(model.training_loss, model.trainable_variables)\n",
    "\n",
    "    # Prediction for visualization\n",
    "    X_new = tf.convert_to_tensor(np.arange(num_nodes).reshape(-1, 1), dtype=tf.float64)  # New input features for prediction\n",
    "    mean, variance = model.predict_f(X_new)  # Predict mean and variance\n",
    "    stddev = tf.sqrt(variance)  # Standard deviation\n",
    "\n",
    "    # Create side-by-side plots for GP and network graph\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Gaussian Process plot on the left\n",
    "    ax[0].plot(X.numpy(), Y.numpy(), 'ro', label='Noisy Samples')\n",
    "    ax[0].plot(X_new.numpy(), mean.numpy(), 'b-', label='Fitted Mean')\n",
    "    ax[0].fill_between(X_new.numpy().flatten(),\n",
    "                       (mean - 1.96 * stddev).numpy().flatten(),\n",
    "                       (mean + 1.96 * stddev).numpy().flatten(),\n",
    "                       color='lightblue', alpha=0.5, label='95% Confidence Interval')\n",
    "    ax[0].set_title(f'Gaussian Process Fit with Graph Diffusion Kernel (beta={beta_sample:.2f})')\n",
    "    ax[0].set_xlabel('Node Number')\n",
    "    ax[0].set_ylabel('Sampled Value')\n",
    "    ax[0].grid()\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # Network graph plot on the right\n",
    "    plot_network_graph(adjacency_matrix, ax[1])\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Learned beta:\", model.kernel.beta.numpy())\n",
    "\n",
    "# Function to plot the network graph in Matplotlib\n",
    "def plot_network_graph(adjacency_matrix, ax):\n",
    "    # Create a NetworkX graph from the adjacency matrix\n",
    "    G = nx.from_numpy_array(adjacency_matrix)\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=300, node_color='red')\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='blue')\n",
    "    \n",
    "    ax.set_title(\"Network Graph Representation of Adjacency Matrix\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Create a slider for beta\n",
    "beta_slider = widgets.FloatSlider(value=0.2, min=0.05, max=2.0, step=0.05, description='Beta:')\n",
    "ui = widgets.VBox([beta_slider])\n",
    "\n",
    "# Link the slider to the plotting function\n",
    "out = widgets.interactive_output(plot_results, {'beta_sample': beta_slider})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9afb90ebd364cf19e7fac0a0641602b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=1.0, description='Beta:', max=2.0, min=0.05, step=0.05),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fa27119b3143d6bdc5bf0ac3ad25d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "# Function to compute the exact diffusion kernel\n",
    "def diffusion_kernel(adj_matrix, beta):\n",
    "    laplacian = np.diag(np.sum(adj_matrix, axis=1)) - adj_matrix  # Graph Laplacian\n",
    "    return expm(-beta * laplacian)  # Matrix exponential\n",
    "\n",
    "# Generate noisy samples function\n",
    "def generate_noisy_samples(adj_matrix, beta_sample):\n",
    "    K = diffusion_kernel(adj_matrix, beta_sample)\n",
    "    L = np.linalg.cholesky(K + 1e-6 * np.eye(adj_matrix.shape[0]))  # Cholesky decomposition\n",
    "    true_samples = L @ np.random.normal(size=(adj_matrix.shape[0], 1))  # Sample from GP\n",
    "    noise = 0.1 * np.random.randn(adj_matrix.shape[0], 1)  # Additive noise\n",
    "    return true_samples + noise  # Noisy observations\n",
    "\n",
    "# Taylor Expansion Diffusion Kernel class\n",
    "class TaylorExpansionDiffusionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, adjacency_matrix, num_terms=15, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.adjacency_matrix = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float64)\n",
    "        self.beta = gpflow.Parameter(2.0, transform=gpflow.utilities.positive())  # Learnable parameter\n",
    "        self.num_terms = num_terms\n",
    "\n",
    "    def K(self, X1, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X1\n",
    "        kernel_matrix = self.taylor_expansion_diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X1 = tf.cast(tf.reshape(X1, [-1]), dtype=tf.int32)\n",
    "        indices_X2 = tf.cast(tf.reshape(X2, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.gather(kernel_matrix, indices_X1, axis=0), indices_X2, axis=1)\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        kernel_matrix = self.taylor_expansion_diffusion_kernel(self.adjacency_matrix, self.beta)\n",
    "        indices_X = tf.cast(tf.reshape(X, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.linalg.diag_part(kernel_matrix), indices_X)\n",
    "\n",
    "    def get_laplacian_matrix(self, W):\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        return D - W\n",
    "\n",
    "    def taylor_expansion_diffusion_kernel(self, adj_matrix, beta):\n",
    "        laplacian = self.get_laplacian_matrix(adj_matrix)\n",
    "        K = np.eye(adj_matrix.shape[0])\n",
    "        for i in range(1, self.num_terms):\n",
    "            K += (-beta)**i * np.linalg.matrix_power(laplacian, i) / math.factorial(i)\n",
    "        return tf.convert_to_tensor(K, dtype=tf.float64)\n",
    "\n",
    "# Exact Diffusion Kernel class\n",
    "class GraphDiffusionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, adjacency_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.adjacency_matrix = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float64)\n",
    "        self.beta = gpflow.Parameter(2.0, transform=gpflow.utilities.positive())\n",
    "\n",
    "    def K(self, X1, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X1\n",
    "        kernel_matrix = diffusion_kernel(self.adjacency_matrix, self.beta.numpy())\n",
    "        indices_X1 = tf.cast(tf.reshape(X1, [-1]), dtype=tf.int32)\n",
    "        indices_X2 = tf.cast(tf.reshape(X2, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.gather(kernel_matrix, indices_X1, axis=0), indices_X2, axis=1)\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        kernel_matrix = diffusion_kernel(self.adjacency_matrix, self.beta.numpy())\n",
    "        indices_X = tf.cast(tf.reshape(X, [-1]), dtype=tf.int32)\n",
    "        return tf.gather(tf.linalg.diag_part(kernel_matrix), indices_X)\n",
    "\n",
    "# Plotting function\n",
    "def plot_results(beta_sample):\n",
    "    clear_output(wait=True)\n",
    "    Y_noisy = generate_noisy_samples(adjacency_matrix, beta_sample)\n",
    "    X = tf.convert_to_tensor(np.arange(num_nodes, dtype=np.float64).reshape(-1, 1))\n",
    "    Y = tf.convert_to_tensor(Y_noisy, dtype=tf.float64)\n",
    "\n",
    "    # Taylor-expanded kernel GP model\n",
    "    taylor_kernel = TaylorExpansionDiffusionKernel(adjacency_matrix)\n",
    "    model_taylor = gpflow.models.GPR(data=(X, Y), kernel=taylor_kernel, mean_function=None)\n",
    "    gpflow.optimizers.Scipy().minimize(model_taylor.training_loss, model_taylor.trainable_variables)\n",
    "\n",
    "    # Exact diffusion kernel GP model\n",
    "    exact_kernel = GraphDiffusionKernel(adjacency_matrix)\n",
    "    model_exact = gpflow.models.GPR(data=(X, Y), kernel=exact_kernel, mean_function=None)\n",
    "    gpflow.optimizers.Scipy().minimize(model_exact.training_loss, model_exact.trainable_variables)\n",
    "\n",
    "    # Predictions for both models\n",
    "    X_new = tf.convert_to_tensor(np.arange(num_nodes).reshape(-1, 1), dtype=tf.float64)\n",
    "    mean_taylor, var_taylor = model_taylor.predict_f(X_new)\n",
    "    mean_exact, var_exact = model_exact.predict_f(X_new)\n",
    "    stddev_taylor = tf.sqrt(var_taylor)\n",
    "    stddev_exact = tf.sqrt(var_exact)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Taylor-expanded kernel plot\n",
    "    ax[0].plot(X.numpy(), Y.numpy(), 'ro', label='Noisy Samples')\n",
    "    ax[0].plot(X_new.numpy(), mean_taylor.numpy(), 'b-', label='Fitted Mean')\n",
    "    ax[0].fill_between(X_new.numpy().flatten(),\n",
    "                       (mean_taylor - 1.96 * stddev_taylor).numpy().flatten(),\n",
    "                       (mean_taylor + 1.96 * stddev_taylor).numpy().flatten(),\n",
    "                       color='lightblue', alpha=0.5, label='95% CI')\n",
    "    ax[0].set_title(\"Taylor-Expanded Kernel GP\")\n",
    "    ax[0].set_xlabel('Node Number')\n",
    "    ax[0].set_ylabel('Sampled Value')\n",
    "    ax[0].grid()\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Exact diffusion kernel plot\n",
    "    ax[1].plot(X.numpy(), Y.numpy(), 'ro', label='Noisy Samples')\n",
    "    ax[1].plot(X_new.numpy(), mean_exact.numpy(), 'g-', label='Fitted Mean')\n",
    "    ax[1].fill_between(X_new.numpy().flatten(),\n",
    "                       (mean_exact - 1.96 * stddev_exact).numpy().flatten(),\n",
    "                       (mean_exact + 1.96 * stddev_exact).numpy().flatten(),\n",
    "                       color='lightgreen', alpha=0.5, label='95% CI')\n",
    "    ax[1].set_title(\"Exact Diffusion Kernel GP\")\n",
    "    ax[1].set_xlabel('Node Number')\n",
    "    ax[1].set_ylabel('Sampled Value')\n",
    "    ax[1].grid()\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Network graph plot\n",
    "    plot_network_graph(adjacency_matrix, ax[2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Network graph plot function\n",
    "def plot_network_graph(adjacency_matrix, ax):\n",
    "    G = nx.from_numpy_array(adjacency_matrix)\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=300, node_color='red')\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='blue')\n",
    "    ax.set_title(\"Network Graph Representation\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 40\n",
    "adjacency_matrix = np.eye(num_nodes, k=1) + np.eye(num_nodes, k=-1)  # Circular adjacency matrix\n",
    "\n",
    "# Slider for beta parameter\n",
    "beta_slider = widgets.FloatSlider(value=1.0, min=0.05, max=2.0, step=0.05, description='Beta:')\n",
    "ui = widgets.VBox([beta_slider])\n",
    "\n",
    "# Interactive output\n",
    "out = widgets.interactive_output(plot_results, {'beta_sample': beta_slider})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G py\n",
    "\n",
    "# GPflow\n",
    "\n",
    "# GP Jax\n",
    "\n",
    "\n",
    "# constained optimization - positive values - minimize() still works\n",
    "\n",
    "\n",
    "\n",
    "# f (modulation function) exist  - postitive definite kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
