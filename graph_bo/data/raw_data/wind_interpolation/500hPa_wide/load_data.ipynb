{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c015c71d",
   "metadata": {},
   "source": [
    "# Wind Data Loading and Preprocessing - 500hPa (Downsampled)\n",
    "\n",
    "This notebook loads and preprocesses ERA5 wind data at 500hPa pressure level with **10x downsampling** for efficient graph-based wind interpolation. The downsampling reduces computational load from ~1M nodes to ~10K nodes while maintaining essential spatial patterns.\n",
    "\n",
    "**Key Features:**\n",
    "- 10x downsampling in both lat/lon directions (721×1440 → 73×144 grid)\n",
    "- Consistent node indexing for graph-based methods\n",
    "- Normalized wind speeds (mean=0, std=1) \n",
    "- Aeolus satellite track for training data\n",
    "- Sparse adjacency matrix with geodesic edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9baf7906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Pressure Level: 500 hPa\n",
      "  Downsampling Factor: 10x\n",
      "  Expected grid reduction: ~100x fewer nodes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from datetime import datetime, timedelta\n",
    "from skyfield.api import load, EarthSatellite, wgs84, utc\n",
    "\n",
    "# Configuration parameters\n",
    "DOWNSAMPLE_FACTOR = 10      # Downsample by factor of 10 in both lat/lon directions\n",
    "PRESSURE_LEVEL = 500        # hPa pressure level\n",
    "PRESSURE_INDEX = 0          # 500hPa is index 0 in the dataset\n",
    "DATA_FILE = '../8176c14c59fd8dc32a74a89b926cb7fd.nc'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Pressure Level: {PRESSURE_LEVEL} hPa\")\n",
    "print(f\"  Downsampling Factor: {DOWNSAMPLE_FACTOR}x\")\n",
    "print(f\"  Expected grid reduction: ~100x fewer nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baaf70e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables: ['number', 'valid_time', 'pressure_level', 'latitude', 'longitude', 'expver', 'u', 'v']\n",
      "\n",
      "Data shapes:\n",
      "  Latitude: (721,)\n",
      "  Longitude: (1440,)\n",
      "  U component: (721, 1440)\n",
      "  V component: (721, 1440)\n",
      "  Original grid size: 721 × 1440 = 1,038,240 points\n",
      "\n",
      "Data shapes:\n",
      "  Latitude: (721,)\n",
      "  Longitude: (1440,)\n",
      "  U component: (721, 1440)\n",
      "  V component: (721, 1440)\n",
      "  Original grid size: 721 × 1440 = 1,038,240 points\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD NETCDF WIND DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load the NetCDF dataset\n",
    "dataset = Dataset(DATA_FILE, mode=\"r\")\n",
    "print(\"Available variables:\", list(dataset.variables.keys()))\n",
    "\n",
    "# Load coordinate arrays\n",
    "lat = dataset.variables[\"latitude\"][:]      # shape (721,)\n",
    "lon = dataset.variables[\"longitude\"][:]     # shape (1440,)\n",
    "\n",
    "# Load wind components (eastward and northward)\n",
    "# Dimensions: (valid_time=12, pressure_level=3, latitude=721, longitude=1440)\n",
    "u = dataset.variables[\"u\"][:]   # eastward wind\n",
    "v = dataset.variables[\"v\"][:]   # northward wind\n",
    "\n",
    "# Extract wind components for 500hPa pressure level\n",
    "u_500 = u[0, PRESSURE_INDEX, :, :]  # shape (721, 1440)\n",
    "v_500 = v[0, PRESSURE_INDEX, :, :]  # shape (721, 1440)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Latitude: {lat.shape}\")\n",
    "print(f\"  Longitude: {lon.shape}\")\n",
    "print(f\"  U component: {u_500.shape}\")\n",
    "print(f\"  V component: {v_500.shape}\")\n",
    "print(f\"  Original grid size: {lat.shape[0]} × {lon.shape[0]} = {lat.shape[0] * lon.shape[0]:,} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0dfb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling Results:\n",
      "  Original grid: 721 × 1440 = 1,038,240 points\n",
      "  Downsampled grid: 73 × 144 = 10,512 points\n",
      "  Reduction factor: 98.8x smaller\n",
      "  Resolution: -2.50° lat × 2.50° lon\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APPLY DOWNSAMPLING\n",
    "# =============================================================================\n",
    "\n",
    "def downsample_grid_data(lat, lon, u_data, v_data, factor=10):\n",
    "    \"\"\"\n",
    "    Downsample the lat/lon grid and corresponding data by a given factor\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lat : array\n",
    "        Latitude coordinates\n",
    "    lon : array  \n",
    "        Longitude coordinates\n",
    "    u_data : array\n",
    "        U wind component data (lat, lon)\n",
    "    v_data : array\n",
    "        V wind component data (lat, lon)\n",
    "    factor : int\n",
    "        Downsampling factor\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lat_down, lon_down, u_down, v_down : downsampled arrays\n",
    "    \"\"\"\n",
    "    # Downsample coordinates\n",
    "    lat_down = lat[::factor]\n",
    "    lon_down = lon[::factor]\n",
    "    \n",
    "    # Downsample data using the same indices\n",
    "    u_down = u_data[::factor, ::factor]\n",
    "    v_down = v_data[::factor, ::factor]\n",
    "    \n",
    "    print(f\"Downsampling Results:\")\n",
    "    print(f\"  Original grid: {lat.shape[0]} × {lon.shape[0]} = {len(lat) * len(lon):,} points\")\n",
    "    print(f\"  Downsampled grid: {lat_down.shape[0]} × {lon_down.shape[0]} = {len(lat_down) * len(lon_down):,} points\")\n",
    "    print(f\"  Reduction factor: {(len(lat) * len(lon)) / (len(lat_down) * len(lon_down)):.1f}x smaller\")\n",
    "    print(f\"  Resolution: {(lat_down[1] - lat_down[0]):.2f}° lat × {(lon_down[1] - lon_down[0]):.2f}° lon\")\n",
    "    \n",
    "    return lat_down, lon_down, u_down, v_down\n",
    "\n",
    "# Apply downsampling\n",
    "lat_processed, lon_processed, u_500_processed, v_500_processed = downsample_grid_data(\n",
    "    lat, lon, u_500, v_500, factor=DOWNSAMPLE_FACTOR\n",
    ")\n",
    "\n",
    "# Calculate wind speed magnitude for the downsampled data\n",
    "wind_speed_processed = np.sqrt(u_500_processed**2 + v_500_processed**2)\n",
    "\n",
    "# Save the downsampled data\n",
    "np.savez(f'wind_data_{PRESSURE_LEVEL}hPa_downsampled.npz', \n",
    "         lon=lon_processed, lat=lat_processed, \n",
    "         u=u_500_processed, v=v_500_processed, wind_speed=wind_speed_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31700c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Aeolus track:\n",
      "  Duration: 24 hours\n",
      "  Sampling: 1 minute intervals\n",
      "  Total points: 1441\n",
      "\n",
      "Snapped to downsampled grid:\n",
      "  Grid resolution: 73 × 144\n",
      "  Unique training locations: 1424\n",
      "\n",
      "Sample track points:\n",
      "                       time   lat    lon\n",
      "0 2019-01-01 09:00:00+00:00  40.0  165.0\n",
      "1 2019-01-01 09:01:00+00:00  42.5  165.0\n",
      "2 2019-01-01 09:02:00+00:00  47.5  162.5\n",
      "3 2019-01-01 09:03:00+00:00  52.5  160.0\n",
      "4 2019-01-01 09:04:00+00:00  57.5  160.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GENERATE AEOLUS SATELLITE TRACK\n",
    "# =============================================================================\n",
    "\n",
    "# Define Aeolus TLE (Two-Line Element) data\n",
    "line1 = \"1 43600U 18066A   21153.73585495  .00031128  00000-0  12124-3 0  9990\"\n",
    "line2 = \"2 43600  96.7150 160.8035 0006915  90.4181 269.7884 15.87015039160910\"\n",
    "\n",
    "ts = load.timescale()\n",
    "aeolus = EarthSatellite(line1, line2, \"AEOLUS\", ts)\n",
    "\n",
    "# Generate times (every minute for 24h starting Jan 1, 2019 09:00 UTC)\n",
    "start = datetime(2019, 1, 1, 9, tzinfo=utc)\n",
    "stop = start + timedelta(hours=24)\n",
    "step = timedelta(minutes=1)\n",
    "\n",
    "times = []\n",
    "t = start\n",
    "while t <= stop:\n",
    "    times.append(t)\n",
    "    t += step\n",
    "\n",
    "# Propagate orbit to get lat/lon\n",
    "geocentric = aeolus.at(ts.from_datetimes(times))\n",
    "sat_lat, sat_lon = wgs84.latlon_of(geocentric)\n",
    "\n",
    "# Convert to arrays in degrees\n",
    "sat_lat = sat_lat.degrees\n",
    "sat_lon = sat_lon.degrees % 360  # wrap to [0,360)\n",
    "\n",
    "raw_track = pd.DataFrame({\"time\": times, \"lat\": sat_lat, \"lon\": sat_lon})\n",
    "\n",
    "print(f\"Generated Aeolus track:\")\n",
    "print(f\"  Duration: 24 hours\")\n",
    "print(f\"  Sampling: 1 minute intervals\") \n",
    "print(f\"  Total points: {len(times)}\")\n",
    "\n",
    "# Snap to downsampled ERA5 grid\n",
    "def snap_to_grid(lat_val, lon_val, era5_lat, era5_lon):\n",
    "    i = np.abs(era5_lat - lat_val).argmin()\n",
    "    j = np.abs(era5_lon - lon_val).argmin()\n",
    "    return era5_lat[i], era5_lon[j]\n",
    "\n",
    "snapped = [snap_to_grid(phi, lam, lat_processed, lon_processed) for phi, lam in zip(sat_lat, sat_lon)]\n",
    "snap_lat, snap_lon = zip(*snapped)\n",
    "\n",
    "snapped_track = pd.DataFrame({\"time\": times, \"lat\": snap_lat, \"lon\": snap_lon})\n",
    "\n",
    "print(f\"\\nSnapped to downsampled grid:\")\n",
    "print(f\"  Grid resolution: {len(lat_processed)} × {len(lon_processed)}\")\n",
    "print(f\"  Unique training locations: {len(set(zip(snap_lat, snap_lon)))}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample track points:\")\n",
    "print(snapped_track.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96f72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD SPHERE GRID GRAPH\n",
    "# =============================================================================\n",
    "\n",
    "# Utilities for spherical coordinates and graph construction\n",
    "def deg2rad(x):\n",
    "    return np.deg2rad(x)\n",
    "\n",
    "def sph2cart(lat_deg, lon_deg, r=1.0):\n",
    "    \"\"\"Convert geographic coords (degrees) to 3D unit-sphere (x,y,z)\"\"\"\n",
    "    lat = deg2rad(lat_deg)\n",
    "    lon = deg2rad(lon_deg)\n",
    "    x = r * np.cos(lat) * np.cos(lon)\n",
    "    y = r * np.cos(lat) * np.sin(lon)\n",
    "    z = r * np.sin(lat)\n",
    "    return np.stack([x, y, z], axis=-1)\n",
    "\n",
    "def great_circle_distance(lat1_deg, lon1_deg, lat2_deg, lon2_deg, R=1.0):\n",
    "    \"\"\"Great-circle distance using the haversine formula\"\"\"\n",
    "    lat1, lon1 = deg2rad(lat1_deg), deg2rad(lon1_deg)\n",
    "    lat2, lon2 = deg2rad(lat2_deg), deg2rad(lon2_deg)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2.0 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "    return R * c\n",
    "\n",
    "def grid_index(i, j, n_lat, n_lon):\n",
    "    \"\"\"Flatten (i,j) -> node id\"\"\"\n",
    "    return i * n_lon + j\n",
    "\n",
    "def inverse_grid_index(node_id, n_lat, n_lon):\n",
    "    \"\"\"Unflatten node id -> (i,j)\"\"\"\n",
    "    i = node_id // n_lon\n",
    "    j = node_id % n_lon\n",
    "    return i, j\n",
    "\n",
    "def build_sphere_grid_graph(lat, lon, connectivity=4, weight=\"geodesic\", radius=1.0):\n",
    "    \"\"\"\n",
    "    Build a mesh-graph over a regular lat-lon grid on the sphere.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    G : networkx.Graph\n",
    "        Nodes [0..N_lat*N_lon-1] with attributes: lat, lon (degrees), xyz: (x,y,z) on unit sphere\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        Weighted adjacency (symmetric)\n",
    "    \"\"\"\n",
    "    n_lat = len(lat)\n",
    "    n_lon = len(lon)\n",
    "\n",
    "    # Precompute per-node attributes\n",
    "    Lon_grid, Lat_grid = np.meshgrid(lon, lat)\n",
    "    xyz = sph2cart(Lat_grid.ravel(), Lon_grid.ravel(), r=1.0)\n",
    "\n",
    "    # Build edges\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    # 4-connected neighbors: north/south/east/west\n",
    "    nbrs_4 = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            nid = grid_index(i, j, n_lat, n_lon)\n",
    "            for di, dj in nbrs_4:\n",
    "                ii = i + di\n",
    "                jj = (j + dj) % n_lon  # periodic in longitude\n",
    "                if 0 <= ii < n_lat:\n",
    "                    nid2 = grid_index(ii, jj, n_lat, n_lon)\n",
    "\n",
    "                    if weight == \"geodesic\":\n",
    "                        w = great_circle_distance(lat[i], lon[j], lat[ii], lon[jj], R=radius)\n",
    "                    else:\n",
    "                        w = 1.0\n",
    "\n",
    "                    rows.append(nid)\n",
    "                    cols.append(nid2)\n",
    "                    data.append(w)\n",
    "\n",
    "    # Create symmetric adjacency matrix\n",
    "    A = sparse.coo_matrix((data, (rows, cols)), shape=(n_lat*n_lon, n_lat*n_lon))\n",
    "    A = ((A + A.T) * 0.5).tocsr()\n",
    "\n",
    "    # Build networkx graph from adjacency\n",
    "    G = nx.from_scipy_sparse_array(A)\n",
    "    \n",
    "    # Attach node attributes\n",
    "    node_attrs = {}\n",
    "    for nid in range(n_lat * n_lon):\n",
    "        i, j = inverse_grid_index(nid, n_lat, n_lon)\n",
    "        node_attrs[nid] = {\n",
    "            \"lat\": float(lat[i]),\n",
    "            \"lon\": float(lon[j]),\n",
    "            \"xyz\": tuple(xyz[nid]),\n",
    "        }\n",
    "    nx.set_node_attributes(G, node_attrs)\n",
    "\n",
    "    return G, A\n",
    "\n",
    "def nearest_node_indices_for_track(track_lat, track_lon, lat, lon):\n",
    "    \"\"\"Given a track of points (lat, lon), return nearest grid (i,j) for each\"\"\"\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    track_lat = np.asarray(track_lat)\n",
    "    track_lon = np.asarray(track_lon) % 360.0\n",
    "\n",
    "    # nearest indices by absolute difference\n",
    "    i_idx = np.abs(track_lat[:, None] - lat[None, :]).argmin(axis=1)\n",
    "    j_idx = np.abs(track_lon[:, None] - lon[None, :]).argmin(axis=1)\n",
    "\n",
    "    node_ids = i_idx * len(lon) + j_idx\n",
    "    idx_ij = np.stack([i_idx, j_idx], axis=1)\n",
    "    return idx_ij, node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0050c5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Construction Results:\n",
      "  Number of nodes: 10,512\n",
      "  Number of edges: 20,880\n",
      "  Adjacency matrix shape: (10512, 10512)\n",
      "  Matrix density: 0.000378\n",
      "  Aeolus track points: 1,441\n",
      "  Unique training nodes: 1,424\n"
     ]
    }
   ],
   "source": [
    "# Build graph using downsampled coordinates\n",
    "G, A = build_sphere_grid_graph(lat_processed, lon_processed, connectivity=4, weight=\"geodesic\", radius=1.0)\n",
    "\n",
    "# Snap Aeolus track to the downsampled grid\n",
    "idx_ij, node_ids = nearest_node_indices_for_track(\n",
    "    track_lat=snapped_track[\"lat\"].values,\n",
    "    track_lon=snapped_track[\"lon\"].values,\n",
    "    lat=lat_processed, \n",
    "    lon=lon_processed,\n",
    ")\n",
    "\n",
    "# Get 3D coordinates for track visualization\n",
    "xyz_track = np.array([G.nodes[n][\"xyz\"] for n in node_ids])\n",
    "\n",
    "print(f\"Graph Construction Results:\")\n",
    "print(f\"  Number of nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"  Number of edges: {G.number_of_edges():,}\")\n",
    "print(f\"  Adjacency matrix shape: {A.shape}\")\n",
    "print(f\"  Matrix density: {A.nnz / (A.shape[0] * A.shape[1]):.6f}\")\n",
    "print(f\"  Aeolus track points: {len(node_ids):,}\")\n",
    "print(f\"  Unique training nodes: {len(np.unique(node_ids)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcdafc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing final datasets for training...\n",
      "Wind speed normalization:\n",
      "  Original: mean=4.096 m/s, std=2.860 m/s\n",
      "  Normalized: mean=-0.000000, std=1.000000\n",
      "  Range: [-1.432, 3.164]\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total nodes: 10,512\n",
      "  Training nodes: 1,424\n",
      "  Training coverage: 13.546%\n",
      "\n",
      "✅ All datasets saved to 'wind_data_processed_500hPa_wide.npz'\n",
      "✅ Downsampling: 10x reduction\n",
      "✅ Grid size: 73 × 144 = 10,512 nodes\n",
      "✅ Training points: 1,424\n",
      "✅ Data normalized: mean=0, std=1\n",
      "\n",
      "✅ All datasets saved to 'wind_data_processed_500hPa_wide.npz'\n",
      "✅ Downsampling: 10x reduction\n",
      "✅ Grid size: 73 × 144 = 10,512 nodes\n",
      "✅ Training points: 1,424\n",
      "✅ Data normalized: mean=0, std=1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE FINAL DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Preparing final datasets for training...\")\n",
    "\n",
    "# Use the downsampled wind data\n",
    "lat_grid = lat_processed\n",
    "lon_grid = lon_processed  \n",
    "u_component = u_500_processed\n",
    "v_component = v_500_processed\n",
    "\n",
    "# Create adjacency matrix A (already computed above as sparse CSR)\n",
    "A = A.tocsr()  # Ensure CSR format\n",
    "\n",
    "# Create node indices (consistent with graph construction)\n",
    "n_lat, n_lon = len(lat_grid), len(lon_grid)\n",
    "X = np.arange(n_lat * n_lon)  # Node indices [0, 1, 2, ..., n_nodes-1]\n",
    "\n",
    "# Create wind speed targets with CONSISTENT indexing\n",
    "y = np.zeros(n_lat * n_lon)\n",
    "coord_mapping = np.zeros((n_lat * n_lon, 2))\n",
    "\n",
    "for i in range(n_lat):\n",
    "    for j in range(n_lon):\n",
    "        node_id = i * n_lon + j  # Same indexing as in graph construction\n",
    "        \n",
    "        # Extract wind components at this grid point\n",
    "        u_val = u_component[i, j]\n",
    "        v_val = v_component[i, j]\n",
    "        wind_speed = np.sqrt(u_val**2 + v_val**2)\n",
    "        \n",
    "        # Assign to correct node index\n",
    "        y[node_id] = wind_speed\n",
    "        \n",
    "        # Store coordinate mapping for reference\n",
    "        coord_mapping[node_id, 0] = lat_grid[i]  # latitude\n",
    "        coord_mapping[node_id, 1] = lon_grid[j]  # longitude\n",
    "\n",
    "# Apply normalization (zero mean, unit variance)\n",
    "y_raw = y.copy()  # Keep original for reference\n",
    "y_mean = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "y = (y - y_mean) / y_std  # Normalize\n",
    "\n",
    "print(f\"Wind speed normalization:\")\n",
    "print(f\"  Original: mean={y_mean:.3f} m/s, std={y_std:.3f} m/s\")\n",
    "print(f\"  Normalized: mean={np.mean(y):.6f}, std={np.std(y):.6f}\")\n",
    "print(f\"  Range: [{np.min(y):.3f}, {np.max(y):.3f}]\")\n",
    "\n",
    "# Create training locations (Aeolus satellite track node indices)\n",
    "unique_train_nodes = np.unique(node_ids)\n",
    "X_train = unique_train_nodes  # Training node indices\n",
    "y_train = y[X_train]  # Corresponding normalized wind speeds\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total nodes: {len(X):,}\")\n",
    "print(f\"  Training nodes: {len(X_train):,}\")\n",
    "print(f\"  Training coverage: {len(X_train)/len(X)*100:.3f}%\")\n",
    "\n",
    "# Also save original u,v components with consistent indexing\n",
    "u_flat_consistent = np.zeros(n_lat * n_lon)\n",
    "v_flat_consistent = np.zeros(n_lat * n_lon)\n",
    "\n",
    "for i in range(n_lat):\n",
    "    for j in range(n_lon):\n",
    "        node_id = i * n_lon + j\n",
    "        u_flat_consistent[node_id] = u_component[i, j]\n",
    "        v_flat_consistent[node_id] = v_component[i, j]\n",
    "\n",
    "# Save the prepared datasets\n",
    "np.savez(f'wind_data_processed_{PRESSURE_LEVEL}hPa_wide.npz',\n",
    "         A_data=A.data,\n",
    "         A_indices=A.indices, \n",
    "         A_indptr=A.indptr,\n",
    "         A_shape=A.shape,\n",
    "         X=X,\n",
    "         y=y,  # Normalized wind speeds\n",
    "         y_mean=y_mean,  # Normalization parameters\n",
    "         y_std=y_std,\n",
    "         X_train=X_train,\n",
    "         y_train=y_train,\n",
    "         coord_mapping=coord_mapping,\n",
    "         u_component=u_flat_consistent,\n",
    "         v_component=v_flat_consistent,\n",
    "         downsample_factor=DOWNSAMPLE_FACTOR,\n",
    "         pressure_level=PRESSURE_LEVEL)\n",
    "\n",
    "print(f\"\\n✅ All datasets saved to 'wind_data_processed_{PRESSURE_LEVEL}hPa_wide.npz'\")\n",
    "print(f\"✅ Downsampling: {DOWNSAMPLE_FACTOR}x reduction\")\n",
    "print(f\"✅ Grid size: {n_lat} × {n_lon} = {n_lat * n_lon:,} nodes\")\n",
    "print(f\"✅ Training points: {len(X_train):,}\")\n",
    "print(f\"✅ Data normalized: mean=0, std=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b6953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
