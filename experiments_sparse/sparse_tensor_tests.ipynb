{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ccb15f",
   "metadata": {},
   "source": [
    "# Sparse CSR Tensor Tests for M2 Mac\n",
    "\n",
    "This notebook tests various sparse CSR tensor operations and autograd compatibility on M2 Mac hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fff83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: macOS-15.5-arm64-arm-64bit\n",
      "PyTorch version: 2.7.1\n",
      "Python version: 3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "MPS available: True\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else 'N/A'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d59080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Sparse Tensor Creation ===\n",
      "✓ COO tensor creation successful\n",
      "✓ CSR tensor creation successful\n",
      "✓ COO to CSR conversion successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/vn4885796ql7_mlpq_dc1n9r0000gn/T/ipykernel_32240/2410557969.py:15: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  sparse_csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, (3, 3))\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic sparse tensor creation\n",
    "print(\"=== Test 1: Basic Sparse Tensor Creation ===\")\n",
    "\n",
    "try:\n",
    "    # COO format\n",
    "    indices = torch.LongTensor([[0, 1, 2], [1, 0, 2]])\n",
    "    values = torch.FloatTensor([1.0, 2.0, 3.0])\n",
    "    sparse_coo = torch.sparse_coo_tensor(indices, values, (3, 3))\n",
    "    print(\"✓ COO tensor creation successful\")\n",
    "    \n",
    "    # CSR format  \n",
    "    crow_indices = torch.LongTensor([0, 1, 2, 3])\n",
    "    col_indices = torch.LongTensor([1, 0, 2])\n",
    "    values = torch.FloatTensor([1.0, 2.0, 3.0])\n",
    "    sparse_csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, (3, 3))\n",
    "    print(\"✓ CSR tensor creation successful\")\n",
    "    \n",
    "    # Convert COO to CSR\n",
    "    sparse_coo_to_csr = sparse_coo.to_sparse_csr()\n",
    "    print(\"✓ COO to CSR conversion successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Sparse tensor creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93055fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 2: Sparse Tensor Arithmetic ===\n",
      "❌ Sparse addition failed: Calling add on a sparse CPU tensor requires compiling PyTorch with MKL. Please use PyTorch built MKL support.\n",
      "✓ Sparse scalar multiplication successful\n",
      "✓ Sparse element-wise multiplication successful\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Sparse tensor arithmetic\n",
    "print(\"\\n=== Test 2: Sparse Tensor Arithmetic ===\")\n",
    "\n",
    "try:\n",
    "    # Create two sparse tensors\n",
    "    indices1 = torch.LongTensor([[0, 1], [1, 0]])\n",
    "    values1 = torch.FloatTensor([1.0, 2.0])\n",
    "    sparse1 = torch.sparse_coo_tensor(indices1, values1, (2, 2)).to_sparse_csr()\n",
    "    \n",
    "    indices2 = torch.LongTensor([[0, 1], [0, 1]])\n",
    "    values2 = torch.FloatTensor([0.5, 1.5])\n",
    "    sparse2 = torch.sparse_coo_tensor(indices2, values2, (2, 2)).to_sparse_csr()\n",
    "    \n",
    "    # Test addition\n",
    "    try:\n",
    "        result_add = sparse1 + sparse2\n",
    "        print(\"✓ Sparse addition successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sparse addition failed: {e}\")\n",
    "    \n",
    "    # Test scalar multiplication\n",
    "    try:\n",
    "        result_mul = sparse1 * 2.0\n",
    "        print(\"✓ Sparse scalar multiplication successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sparse scalar multiplication failed: {e}\")\n",
    "    \n",
    "    # Test element-wise multiplication\n",
    "    try:\n",
    "        result_elem_mul = sparse1 * sparse2\n",
    "        print(\"✓ Sparse element-wise multiplication successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sparse element-wise multiplication failed: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Sparse arithmetic setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ef45eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 3: Sparse-Dense Matrix Multiplication ===\n",
      "✓ Sparse @ dense vector successful\n",
      "  Result shape: torch.Size([3, 1])\n",
      "✓ Sparse @ dense matrix successful\n",
      "  Result shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Sparse-dense matrix multiplication\n",
    "print(\"\\n=== Test 3: Sparse-Dense Matrix Multiplication ===\")\n",
    "\n",
    "try:\n",
    "    # Create sparse matrix\n",
    "    crow_indices = torch.LongTensor([0, 2, 3, 4])\n",
    "    col_indices = torch.LongTensor([0, 2, 1, 0])\n",
    "    values = torch.FloatTensor([1.0, 2.0, 3.0, 4.0])\n",
    "    sparse_matrix = torch.sparse_csr_tensor(crow_indices, col_indices, values, (3, 3))\n",
    "    \n",
    "    # Create dense vector/matrix\n",
    "    dense_vector = torch.randn(3, 1)\n",
    "    dense_matrix = torch.randn(3, 5)\n",
    "    \n",
    "    # Test sparse @ dense\n",
    "    try:\n",
    "        result_vec = torch.sparse.mm(sparse_matrix, dense_vector)\n",
    "        print(\"✓ Sparse @ dense vector successful\")\n",
    "        print(f\"  Result shape: {result_vec.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sparse @ dense vector failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        result_mat = torch.sparse.mm(sparse_matrix, dense_matrix)\n",
    "        print(\"✓ Sparse @ dense matrix successful\")\n",
    "        print(f\"  Result shape: {result_mat.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sparse @ dense matrix failed: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Sparse-dense multiplication setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90e6cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 4: Autograd with Sparse Tensors ===\n",
      "✓ Forward pass with weighted sparse matrix successful\n",
      "  Loss: 171.4059\n",
      "✓ Backward pass successful\n",
      "  Weight gradient: 151.4175\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Autograd with sparse tensors\n",
    "print(\"\\n=== Test 4: Autograd with Sparse Tensors ===\")\n",
    "\n",
    "try:\n",
    "    # Create learnable parameter\n",
    "    weight = torch.tensor(2.0, requires_grad=True)\n",
    "    \n",
    "    # Create sparse matrix (non-learnable structure)\n",
    "    crow_indices = torch.LongTensor([0, 2, 3, 4])\n",
    "    col_indices = torch.LongTensor([0, 2, 1, 0])\n",
    "    values = torch.FloatTensor([1.0, 2.0, 3.0, 4.0])\n",
    "    sparse_matrix = torch.sparse_csr_tensor(crow_indices, col_indices, values, (3, 3))\n",
    "    \n",
    "    # Create dense target\n",
    "    target = torch.randn(3, 1)\n",
    "    \n",
    "    # Forward pass: weight * sparse @ dense\n",
    "    try:\n",
    "        weighted_sparse = sparse_matrix * weight\n",
    "        result = torch.sparse.mm(weighted_sparse, torch.ones(3, 1))\n",
    "        loss = torch.sum((result - target) ** 2)\n",
    "        \n",
    "        print(\"✓ Forward pass with weighted sparse matrix successful\")\n",
    "        print(f\"  Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        print(\"✓ Backward pass successful\")\n",
    "        print(f\"  Weight gradient: {weight.grad.item():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Autograd with sparse tensors failed: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Autograd setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a82f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 5: Multiple Parameters + Sparse Operations ===\n",
      "Testing scipy-based approach:\n",
      "❌ Scipy approach failed: indices expected sparse coordinate tensor layout but got SparseCsr\n",
      "\n",
      "Testing direct PyTorch approach:\n",
      "❌ Direct PyTorch approach failed: Calling add on a sparse CPU tensor requires compiling PyTorch with MKL. Please use PyTorch built MKL support.\n",
      "  This is expected on M2 Mac without MKL\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Multiple learnable parameters with sparse operations\n",
    "print(\"\\n=== Test 5: Multiple Parameters + Sparse Operations ===\")\n",
    "\n",
    "try:\n",
    "    # Create multiple learnable parameters (like modulator vector)\n",
    "    num_matrices = 3\n",
    "    modulator = torch.randn(num_matrices, requires_grad=True)\n",
    "    \n",
    "    # Create multiple sparse matrices\n",
    "    sparse_matrices = []\n",
    "    for i in range(num_matrices):\n",
    "        # Create different sparse patterns\n",
    "        crow_indices = torch.LongTensor([0, 1, 2, 3])\n",
    "        col_indices = torch.LongTensor([i % 3, (i+1) % 3, (i+2) % 3])\n",
    "        values = torch.FloatTensor([1.0, 0.5, 2.0])\n",
    "        sparse_mat = torch.sparse_csr_tensor(crow_indices, col_indices, values, (3, 3))\n",
    "        sparse_matrices.append(sparse_mat)\n",
    "    \n",
    "    # Forward pass: sum of weighted sparse matrices\n",
    "    try:\n",
    "        # Method 1: Using scipy for arithmetic (M2 Mac compatible)\n",
    "        print(\"Testing scipy-based approach:\")\n",
    "        \n",
    "        # Convert to scipy, do arithmetic, convert back\n",
    "        modulator_np = modulator.detach().cpu().numpy()\n",
    "        result_scipy = None\n",
    "        \n",
    "        for i, sparse_torch in enumerate(sparse_matrices):\n",
    "            # Convert to scipy\n",
    "            sparse_torch_cpu = sparse_torch.cpu()\n",
    "            scipy_matrix = sp.csr_matrix((\n",
    "                sparse_torch_cpu.values().numpy(),\n",
    "                sparse_torch_cpu.indices().numpy(),\n",
    "                sparse_torch_cpu.crow_indices().numpy()\n",
    "            ), shape=sparse_torch_cpu.shape)\n",
    "            \n",
    "            # Weight and accumulate\n",
    "            weighted = modulator_np[i] * scipy_matrix\n",
    "            result_scipy = weighted if result_scipy is None : result_scipy + weighted\n",
    "        \n",
    "        # Convert back to torch\n",
    "        crow_indices = torch.from_numpy(result_scipy.indptr).long()\n",
    "        col_indices = torch.from_numpy(result_scipy.indices).long()\n",
    "        values = torch.from_numpy(result_scipy.data).float()\n",
    "        \n",
    "        final_sparse = torch.sparse_csr_tensor(\n",
    "            crow_indices, col_indices, values, result_scipy.shape\n",
    "        )\n",
    "        \n",
    "        # Test matrix multiplication\n",
    "        dense_input = torch.randn(3, 1)\n",
    "        output = torch.sparse.mm(final_sparse, dense_input)\n",
    "        loss = torch.sum(output ** 2)\n",
    "        \n",
    "        print(\"✓ Scipy-based sparse arithmetic successful\")\n",
    "        print(f\"  Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # This won't have gradients since we went through numpy\n",
    "        print(\"  Note: Gradients lost through scipy conversion\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Scipy approach failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Method 2: Direct PyTorch approach (may fail on M2 Mac)\n",
    "        print(\"\\nTesting direct PyTorch approach:\")\n",
    "        \n",
    "        result_torch = None\n",
    "        for i, sparse_mat in enumerate(sparse_matrices):\n",
    "            weighted = sparse_mat * modulator[i]\n",
    "            result_torch = weighted if result_torch is None : result_torch + weighted\n",
    "        \n",
    "        dense_input = torch.randn(3, 1)\n",
    "        output = torch.sparse.mm(result_torch, dense_input)\n",
    "        loss = torch.sum(output ** 2)\n",
    "        \n",
    "        loss.backward()\n",
    "        print(\"✓ Direct PyTorch sparse arithmetic successful\")\n",
    "        print(f\"  Loss: {loss.item():.4f}\")\n",
    "        print(f\"  Modulator gradients: {modulator.grad}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Direct PyTorch approach failed: {e}\")\n",
    "        print(\"  This is expected on M2 Mac without MKL\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Multiple parameters test setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f221727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 6: M2 Mac Compatible Approaches ===\n",
      "Testing hybrid dense-sparse approach:\n",
      "✓ Hybrid approach successful\n",
      "  Loss: 2.4205\n",
      "  Modulator gradients: tensor([ 5.3504,  2.6539, -5.5658])\n",
      "\n",
      "Testing matrix-vector product approach:\n",
      "✓ Matrix-vector product approach successful\n",
      "  Loss: 4.7862\n",
      "  Modulator gradients: tensor([ 2.4408,  2.2795, -5.4320])\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Alternative approaches for M2 Mac\n",
    "print(\"\\n=== Test 6: M2 Mac Compatible Approaches ===\")\n",
    "\n",
    "try:\n",
    "    # Approach 1: Keep gradients by using dense arithmetic selectively\n",
    "    print(\"Testing hybrid dense-sparse approach:\")\n",
    "    \n",
    "    num_matrices = 3\n",
    "    modulator = torch.randn(num_matrices, requires_grad=True)\n",
    "    \n",
    "    # Create sparse matrices as before\n",
    "    sparse_matrices = []\n",
    "    for i in range(num_matrices):\n",
    "        crow_indices = torch.LongTensor([0, 1, 2, 3])\n",
    "        col_indices = torch.LongTensor([i % 3, (i+1) % 3, (i+2) % 3])\n",
    "        values = torch.FloatTensor([1.0, 0.5, 2.0])\n",
    "        sparse_mat = torch.sparse_csr_tensor(crow_indices, col_indices, values, (3, 3))\n",
    "        sparse_matrices.append(sparse_mat)\n",
    "    \n",
    "    # Sum using dense intermediate (small matrices, should be OK)\n",
    "    result_dense = torch.zeros(3, 3)\n",
    "    for i, sparse_mat in enumerate(sparse_matrices):\n",
    "        dense_mat = sparse_mat.to_dense()\n",
    "        result_dense += modulator[i] * dense_mat\n",
    "    \n",
    "    # Convert back to sparse for memory efficiency\n",
    "    result_sparse = result_dense.to_sparse_csr()\n",
    "    \n",
    "    # Matrix multiplication\n",
    "    dense_input = torch.randn(3, 1)\n",
    "    output = torch.sparse.mm(result_sparse, dense_input)\n",
    "    loss = torch.sum(output ** 2)\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"✓ Hybrid approach successful\")\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "    print(f\"  Modulator gradients: {modulator.grad}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Hybrid approach failed: {e}\")\n",
    "\n",
    "try:\n",
    "    # Approach 2: Matrix-vector products without explicit addition\n",
    "    print(\"\\nTesting matrix-vector product approach:\")\n",
    "    \n",
    "    modulator = torch.randn(num_matrices, requires_grad=True)\n",
    "    dense_input = torch.randn(3, 1)\n",
    "    \n",
    "    # Compute sum(modulator[i] * M[i] @ v) directly\n",
    "    result = torch.zeros(3, 1)\n",
    "    for i, sparse_mat in enumerate(sparse_matrices):\n",
    "        mv_product = torch.sparse.mm(sparse_mat, dense_input)\n",
    "        result += modulator[i] * mv_product\n",
    "    \n",
    "    loss = torch.sum(result ** 2)\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"✓ Matrix-vector product approach successful\")\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "    print(f\"  Modulator gradients: {modulator.grad}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Matrix-vector product approach failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce28439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 7: Performance Comparison ===\n",
      "✓ Large scale test successful\n",
      "  Matrix size: 100x100, 5 matrices\n",
      "  Time: 0.0009 seconds\n",
      "  Loss: 1004.4420\n",
      "  Memory efficient: Avoided materializing sum of sparse matrices\n"
     ]
    }
   ],
   "source": [
    "# Test 7: Performance comparison\n",
    "print(\"\\n=== Test 7: Performance Comparison ===\")\n",
    "\n",
    "import time\n",
    "\n",
    "try:\n",
    "    # Setup larger test case\n",
    "    n = 100\n",
    "    num_matrices = 5\n",
    "    modulator = torch.randn(num_matrices, requires_grad=True)\n",
    "    \n",
    "    # Create larger sparse matrices\n",
    "    large_sparse_matrices = []\n",
    "    for i in range(num_matrices):\n",
    "        # Random sparse pattern\n",
    "        nnz = n // 2  # About 50% sparsity\n",
    "        row_indices = torch.randint(0, n, (nnz,))\n",
    "        col_indices = torch.randint(0, n, (nnz,))\n",
    "        values = torch.randn(nnz)\n",
    "        \n",
    "        sparse_coo = torch.sparse_coo_tensor(\n",
    "            torch.stack([row_indices, col_indices]), values, (n, n)\n",
    "        ).coalesce()\n",
    "        sparse_csr = sparse_coo.to_sparse_csr()\n",
    "        large_sparse_matrices.append(sparse_csr)\n",
    "    \n",
    "    dense_input = torch.randn(n, 10)  # Multiple columns\n",
    "    \n",
    "    # Time the matrix-vector product approach\n",
    "    start_time = time.time()\n",
    "    result = torch.zeros(n, 10)\n",
    "    for i, sparse_mat in enumerate(large_sparse_matrices):\n",
    "        mv_product = torch.sparse.mm(sparse_mat, dense_input)\n",
    "        result += modulator[i] * mv_product\n",
    "    \n",
    "    loss = torch.sum(result ** 2)\n",
    "    loss.backward()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"✓ Large scale test successful\")\n",
    "    print(f\"  Matrix size: {n}x{n}, {num_matrices} matrices\")\n",
    "    print(f\"  Time: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "    print(f\"  Memory efficient: Avoided materializing sum of sparse matrices\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Performance test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28f970",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tests various sparse tensor operations on M2 Mac:\n",
    "\n",
    "1. **Basic Operations**: CSR tensor creation and conversion\n",
    "2. **Arithmetic**: Addition, multiplication (may fail due to MKL dependency)\n",
    "3. **Matrix Multiplication**: Sparse @ dense operations (usually work)\n",
    "4. **Autograd**: Gradient computation through sparse operations\n",
    "5. **Workarounds**: M2 Mac compatible approaches:\n",
    "   - Scipy-based arithmetic (loses gradients)\n",
    "   - Hybrid dense-sparse (works for small matrices)\n",
    "   - Matrix-vector products (avoids sparse addition)\n",
    "\n",
    "**Recommended approach for M2 Mac**: Use matrix-vector products to avoid sparse matrix addition while maintaining autograd support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d246c4",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Based on the test results, here's what we learned about sparse tensor operations on M2 Mac:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894374a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of test results based on the variables\n",
    "print(\"=== SPARSE TENSOR TEST RESULTS ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"✅ SUCCESSFUL OPERATIONS:\")\n",
    "print(\"1. Basic tensor creation: COO and CSR tensors work perfectly\")\n",
    "print(\"2. Sparse-dense multiplication: torch.sparse.mm() works reliably\")  \n",
    "print(\"3. Scalar multiplication: sparse * scalar works\")\n",
    "print(\"4. Autograd with scalar weights: Gradients flow correctly\")\n",
    "\n",
    "print(\"\\n❌ PROBLEMATIC OPERATIONS:\")\n",
    "print(\"1. Sparse + Sparse addition: Likely failed (MKL dependency)\")\n",
    "print(\"2. Element-wise sparse multiplication: Limited support\")\n",
    "\n",
    "print(\"\\n🔧 M2 MAC COMPATIBLE SOLUTIONS:\")\n",
    "\n",
    "# Check if hybrid approach worked\n",
    "if 'result_dense' in locals() and result_dense is not None:\n",
    "    print(\"✅ Hybrid dense-sparse approach: SUCCESSFUL\")\n",
    "    print(f\"   - Dense intermediate computation works\")\n",
    "    print(f\"   - Can convert back to sparse for memory efficiency\")\n",
    "\n",
    "# Check if matrix-vector product approach worked  \n",
    "if 'result' in locals() and result.requires_grad:\n",
    "    print(\"✅ Matrix-vector product approach: SUCCESSFUL\")\n",
    "    print(f\"   - Avoids sparse matrix addition entirely\")\n",
    "    print(f\"   - Maintains gradient flow: {result.requires_grad}\")\n",
    "    print(f\"   - Result shape: {result.shape}\")\n",
    "\n",
    "print(\"\\n📊 PERFORMANCE INSIGHTS:\")\n",
    "if 'end_time' in locals() and 'start_time' in locals():\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"✅ Large scale test (100x100 matrices): {elapsed:.4f} seconds\")\n",
    "    print(\"   - Matrix-vector products scale well\")\n",
    "    print(\"   - Memory efficient: no materialized sparse sums\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED APPROACH FOR YOUR KERNEL:\")\n",
    "print(\"Use the matrix-vector product method:\")\n",
    "print(\"  result = sum(weight[i] * sparse_matrix[i] @ dense_vector)\")\n",
    "print(\"This avoids problematic sparse+sparse operations while keeping gradients.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
