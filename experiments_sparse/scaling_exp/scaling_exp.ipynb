{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c21c4e0",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecb9ae",
   "metadata": {},
   "source": [
    "\n",
    "**1. Experimental Configs** \n",
    "\n",
    "Setup for:\n",
    "1. graph sizes to run for sparse / dense\n",
    "2. rw parameters\n",
    "3. gpytorch / linear operator settings\n",
    "4. GPU / CPU\n",
    "5. Random Seeds\n",
    "\n",
    "**2. Data Synthesis**\n",
    "\n",
    "Generate syntheric ring graph data for diff, store in *'experiments_sparse/scaling_exp/synthetic_data'*\n",
    "\n",
    "**3. Random Walk Sampling + Compute Step Matrices**\n",
    "\n",
    "For dense / sparse settings, run the rw sampling scheme multiple times with different seeds:\n",
    "\n",
    "1. Load the synthetic graphs\n",
    "2. Run rw samples for the specified graph sizes\n",
    "3. Store the step matrices pickle files in *'experiments_sparse/scaling_exp/step_matrices'*\n",
    "4. Measure the object sizes / rw timing of step matrices, store the result in *'experiments_sparse/scaling_exp/stats'*\n",
    "\n",
    "**4. Gaussian Processes: Init, Training and Inference**\n",
    "\n",
    "For dense / sparse settings, run the GP model training / inference multiple times with different seeds, across all graph sizes:\n",
    "\n",
    "1. Load the step matrices to init kernels\n",
    "2. Train the model with GPU with a fixed number of iterations, record the timing\n",
    "3. Do inference with GPU, record the inference time\n",
    "4. store the timing results in *'experiments_sparse/scaling_exp/stats'*\n",
    "\n",
    "\n",
    "**Analysis and visualization will be in a different notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88f498",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59667bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Core imports and setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# GP framework imports\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch import settings as gsettings\n",
    "from gpytorch.kernels import MultiDeviceKernel\n",
    "from linear_operator import settings\n",
    "from linear_operator.utils import linear_cg\n",
    "from linear_operator.operators import IdentityLinearOperator\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append('../..')\n",
    "from efficient_graph_gp.random_walk_samplers.sampler import RandomWalk as DenseRandomWalk, Graph as DenseGraph\n",
    "from efficient_graph_gp_sparse.preprocessor import GraphPreprocessor\n",
    "from efficient_graph_gp.gpflow_kernels import GraphGeneralFastGRFKernel\n",
    "from efficient_graph_gp_sparse.gptorch_kernels_sparse.sparse_grf_kernel import SparseGRFKernel\n",
    "from efficient_graph_gp_sparse.utils_sparse import SparseLinearOperator\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213ccdd",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d5e3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "\n",
    "def generate_ring_graph_data(n_nodes, beta_sample=1.0, kernel_std=1.0, noise_std=0.1, \n",
    "                           splits=[0.6, 0.2, 0.2], seed=42):\n",
    "    \"\"\"Generate synthetic data on a ring graph\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create ring graph\n",
    "    G = nx.cycle_graph(n_nodes)\n",
    "    A = nx.adjacency_matrix(G).tocsr()\n",
    "    \n",
    "    # Generate smooth function on ring\n",
    "    angles = np.linspace(0, 2*np.pi, n_nodes, endpoint=False)\n",
    "    y_true = beta_sample * (2*np.sin(2*angles) + 0.5*np.cos(4*angles) + 0.3*np.sin(angles))\n",
    "    y_observed = y_true + np.random.normal(0, noise_std, n_nodes)\n",
    "    \n",
    "    # Create splits\n",
    "    indices = np.arange(n_nodes)\n",
    "    train_size = int(splits[0] * n_nodes)\n",
    "    val_size = int(splits[1] * n_nodes)\n",
    "    \n",
    "    train_idx = np.random.choice(indices, train_size, replace=False)\n",
    "    remaining = np.setdiff1d(indices, train_idx)\n",
    "    val_idx = np.random.choice(remaining, val_size, replace=False)\n",
    "    test_idx = np.setdiff1d(remaining, val_idx)\n",
    "    \n",
    "    return {\n",
    "        'A_sparse': A,\n",
    "        'A_dense': A.toarray().astype(np.float64),\n",
    "        'G': G,\n",
    "        'y_true': y_true,\n",
    "        'y_observed': y_observed,\n",
    "        'X_train': train_idx.reshape(-1, 1).astype(np.float64),\n",
    "        'y_train': y_observed[train_idx].reshape(-1, 1),\n",
    "        'X_val': val_idx.reshape(-1, 1).astype(np.float64),\n",
    "        'y_val': y_observed[val_idx].reshape(-1, 1),\n",
    "        'X_test': test_idx.reshape(-1, 1).astype(np.float64),\n",
    "        'y_test': y_observed[test_idx].reshape(-1, 1),\n",
    "        'train_idx': train_idx,\n",
    "        'val_idx': val_idx,\n",
    "        'test_idx': test_idx\n",
    "    }\n",
    "\n",
    "def save_data(data, filepath):\n",
    "    \"\"\"Save data to pickle file\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from pickle file\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_data_filepath(n_nodes, data_dir, params):\n",
    "    \"\"\"Generate filepath for cached data\"\"\"\n",
    "    filename = f\"ring_n{n_nodes}_beta{params['beta_sample']}_std{params['kernel_std']}_noise{params['noise_std']}_seed{params['seed']}.pkl\"\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "def generate_and_cache_data(n_nodes, data_dir, beta_sample=1.0, kernel_std=1.0, \n",
    "                          noise_std=0.1, splits=[0.6, 0.2, 0.2], seed=42):\n",
    "    \"\"\"Generate data and cache to disk, or load from cache if exists\"\"\"\n",
    "    params = {\n",
    "        'beta_sample': beta_sample,\n",
    "        'kernel_std': kernel_std, \n",
    "        'noise_std': noise_std,\n",
    "        'seed': seed\n",
    "    }\n",
    "    \n",
    "    filepath = get_data_filepath(n_nodes, data_dir, params)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Loading cached data for {n_nodes} nodes...\")\n",
    "        return load_data(filepath)\n",
    "    else:\n",
    "        print(f\"Generating data for {n_nodes} nodes...\")\n",
    "        data = generate_ring_graph_data(n_nodes, beta_sample, kernel_std, noise_std, splits, seed)\n",
    "        save_data(data, filepath)\n",
    "        return data\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Helper function to move data to device\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [to_device(item, device) for item in data]\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def save_experiment_results(df, experiment_name, stats_dir, config_params=None):\n",
    "    \"\"\"\n",
    "    Save experiment results with timestamped files and configuration\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with results\n",
    "        experiment_name: Name for the experiment (e.g., 'sparse_gp_scaling')\n",
    "        stats_dir: Directory to save results\n",
    "        config_params: Optional dict of configuration parameters\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    os.makedirs(stats_dir, exist_ok=True)\n",
    "    \n",
    "    # Save main results\n",
    "    main_file = os.path.join(stats_dir, f'{experiment_name}_stats.csv')\n",
    "    timestamped_file = os.path.join(stats_dir, f'{experiment_name}_stats_{timestamp}.csv')\n",
    "    \n",
    "    df.to_csv(main_file, index=False)\n",
    "    df.to_csv(timestamped_file, index=False)\n",
    "    \n",
    "    # Save configuration if provided\n",
    "    if config_params:\n",
    "        config_summary = {\n",
    "            'timestamp': timestamp,\n",
    "            'total_experiments': len(df),\n",
    "            'experiment_name': experiment_name,\n",
    "            **config_params\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(stats_dir, f'{experiment_name}_config_{timestamp}.json'), 'w') as f:\n",
    "            json.dump(config_summary, f, indent=2)\n",
    "    \n",
    "    # Compute and save summary statistics\n",
    "    if len(df) > 0:\n",
    "        # Group by graph size if 'n_nodes' column exists\n",
    "        if 'n_nodes' in df.columns:\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            summary = df.groupby('n_nodes')[numeric_cols].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "        else:\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            summary = df[numeric_cols].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "        \n",
    "        summary.to_csv(os.path.join(stats_dir, f'{experiment_name}_summary_{timestamp}.csv'))\n",
    "    \n",
    "    print(f\"üìÅ {experiment_name} results saved:\")\n",
    "    print(f\"   Main file: {main_file}\")\n",
    "    print(f\"   Timestamped: {timestamped_file}\")\n",
    "    if config_params:\n",
    "        print(f\"   Config: {experiment_name}_config_{timestamp}.json\")\n",
    "        print(f\"   Summary: {experiment_name}_summary_{timestamp}.csv\")\n",
    "    \n",
    "    return timestamped_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40b5c6",
   "metadata": {},
   "source": [
    "### Exp Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output device: cuda:0, Number of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTAL CONFIGURATION PARAMETERS\n",
    "# =====================================\n",
    "\n",
    "# GPyTorch & Linear Operator settings\n",
    "settings.verbose_linalg._default = False\n",
    "settings._fast_covar_root_decomposition._default = False\n",
    "gsettings.max_cholesky_size._global_value = 0\n",
    "gsettings.cg_tolerance._global_value = 1e-2\n",
    "gsettings.max_lanczos_quadrature_iterations._global_value = 1\n",
    "gsettings.num_trace_samples._global_value = 64\n",
    "gsettings.min_preconditioning_size._global_value = 1e10 #TODO: Enable preconditioning in future\n",
    "\n",
    "# Random Walk Parameters\n",
    "WALKS_PER_NODE = 100\n",
    "P_HALT = 0.1\n",
    "MAX_WALK_LENGTH = 3\n",
    "\n",
    "# GP Graph Sizes\n",
    "GP_GRAPH_SIZES = [2**i for i in range(5, 8)]\n",
    "GP_SPARSE_ONLY_SIZES = [2**i for i in range(8, 11)]\n",
    "\n",
    "# Training Parameters\n",
    "N_EPOCHS = 50\n",
    "TRAIN_RATIO = 0.6\n",
    "NOISE_STD = 0.1 # Noise in synthetic data\n",
    "INITIAL_NOISE_VARIANCE = 0.1\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# Device configuration\n",
    "output_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "n_devices = torch.cuda.device_count()\n",
    "print(f\"Output device: {output_device}, Number of GPUs: {n_devices}\")\n",
    "\n",
    "# Number of Repeats & Random Seeds\n",
    "N_REPEATS = 5\n",
    "RW_SEEDS = [42 + i for i in range(N_REPEATS)]\n",
    "\n",
    "# Data synthesis parameters\n",
    "DATA_SYNTHESIS_PARAMS = {\n",
    "    'beta_sample': 1.0,\n",
    "    'kernel_std': 1.0,\n",
    "    'noise_std': 0.1,\n",
    "    'splits': [0.6, 0.2, 0.2],\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'synthetic_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2f00a",
   "metadata": {},
   "source": [
    "### Data Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c217df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesizing ring graph data for all graph sizes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data for 32 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 265.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data for 64 nodes...\n",
      "Loading cached data for 128 nodes...\n",
      "Loading cached data for 256 nodes...\n",
      "Loading cached data for 512 nodes...\n",
      "Loading cached data for 1024 nodes...\n",
      "Data synthesis complete. Files stored in: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/synthetic_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def synthesize_all_data():\n",
    "    print(\"Synthesizing ring graph data for all graph sizes...\")\n",
    "    \n",
    "    all_sizes = GP_GRAPH_SIZES + GP_SPARSE_ONLY_SIZES\n",
    "    \n",
    "    for n_nodes in tqdm(all_sizes, desc=\"Generating datasets\"):\n",
    "        data = generate_and_cache_data(\n",
    "            n_nodes=n_nodes,\n",
    "            data_dir=DATA_DIR,\n",
    "            **DATA_SYNTHESIS_PARAMS\n",
    "        )\n",
    "        \n",
    "        # Clean up memory for large datasets\n",
    "        del data\n",
    "        if n_nodes >= 10000:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"Data synthesis complete. Files stored in: {DATA_DIR}\")\n",
    "\n",
    "# Run data synthesis\n",
    "synthesize_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfb11a",
   "metadata": {},
   "source": [
    "### RW Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4c56a",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f604e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparse_rw_sampling(data, rw_seed, n_nodes):\n",
    "    \"\"\"Run sparse random walk sampling for a single graph\"\"\"\n",
    "    start_time = time.time()\n",
    "    pp_sparse = GraphPreprocessor(\n",
    "        adjacency_matrix=data['A_sparse'],\n",
    "        walks_per_node=WALKS_PER_NODE,\n",
    "        p_halt=P_HALT,\n",
    "        max_walk_length=MAX_WALK_LENGTH,\n",
    "        random_walk_seed=rw_seed,\n",
    "        load_from_disk=False,\n",
    "        use_tqdm=False,\n",
    "        n_processes=4\n",
    "    )\n",
    "    \n",
    "    # The preprocessor returns torch tensors, but we want scipy matrices\n",
    "    # So we access the scipy matrices directly after preprocessing\n",
    "    pp_sparse.preprocess_graph(save_to_disk=False)\n",
    "    step_matrices_scipy = pp_sparse.step_matrices_scipy\n",
    "    sparse_rw_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate object sizes using scipy matrices\n",
    "    sparse_total_nnz = sum(mat.nnz for mat in step_matrices_scipy)\n",
    "    sparse_size_mb = sparse_total_nnz * 16 / (1024**2)  # 8 bytes for values + 8 for indices\n",
    "    sparse_dense_equiv_mb = sum(mat.shape[0] * mat.shape[1] * 8 for mat in step_matrices_scipy) / (1024**2)\n",
    "    \n",
    "    return {\n",
    "        'time': sparse_rw_time,\n",
    "        'step_matrices': step_matrices_scipy,\n",
    "        'total_nnz': sparse_total_nnz,\n",
    "        'size_mb': sparse_size_mb,\n",
    "        'dense_equiv_mb': sparse_dense_equiv_mb,\n",
    "        'avg_nnz_per_matrix': sparse_total_nnz / len(step_matrices_scipy),\n",
    "        'sparsity': sparse_total_nnz / sum(mat.shape[0] * mat.shape[1] for mat in step_matrices_scipy)\n",
    "    }\n",
    "\n",
    "def run_dense_rw_sampling(data, rw_seed, n_nodes):\n",
    "    \"\"\"Run dense random walk sampling for a single graph\"\"\"\n",
    "    start_time = time.time()\n",
    "    dense_graph = DenseGraph(data['A_dense'])\n",
    "    dense_sampler = DenseRandomWalk(dense_graph, seed=rw_seed)\n",
    "    \n",
    "    dense_step_matrices = dense_sampler.get_random_walk_matrices(\n",
    "        WALKS_PER_NODE, P_HALT, MAX_WALK_LENGTH\n",
    "    )\n",
    "    dense_rw_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate object sizes\n",
    "    dense_size_mb = dense_step_matrices.nbytes / (1024**2)\n",
    "    \n",
    "    return {\n",
    "        'time': dense_rw_time,\n",
    "        'step_matrices': dense_step_matrices,\n",
    "        'size_mb': dense_size_mb\n",
    "    }\n",
    "\n",
    "def save_step_matrices(step_matrices_dir, method, n_nodes, rw_seed, step_matrices, config):\n",
    "    \"\"\"Save step matrices to disk and return file size\"\"\"\n",
    "    filename = f\"step_matrices_{method}_n{n_nodes}_seed{rw_seed}.pkl\"\n",
    "    filepath = os.path.join(step_matrices_dir, filename)\n",
    "    \n",
    "    save_data = {\n",
    "        'step_matrices_torch' if method == 'sparse' else 'step_matrices': step_matrices,\n",
    "        'n_nodes': n_nodes,\n",
    "        'seed': rw_seed,\n",
    "        'method': method,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "    \n",
    "    return os.path.getsize(filepath) / (1024**2)\n",
    "\n",
    "def process_single_graph(n_nodes, data_dir, step_matrices_dir, rw_seeds):\n",
    "    \"\"\"Process a single graph size with all seeds\"\"\"\n",
    "    print(f\"\\nProcessing {n_nodes} nodes...\")\n",
    "    \n",
    "    # Load synthetic data\n",
    "    data = generate_and_cache_data(n_nodes=n_nodes, data_dir=data_dir, **DATA_SYNTHESIS_PARAMS)\n",
    "    \n",
    "    # Determine if we should run dense\n",
    "    run_dense = n_nodes <= 256  # Conservative threshold\n",
    "    \n",
    "    results = []\n",
    "    config = {'walks_per_node': WALKS_PER_NODE, 'p_halt': P_HALT, 'max_walk_length': MAX_WALK_LENGTH}\n",
    "    \n",
    "    for seed_idx, rw_seed in enumerate(rw_seeds):\n",
    "        print(f\"  Seed {seed_idx + 1}/{len(rw_seeds)} (seed={rw_seed})\")\n",
    "        \n",
    "        # Run sparse sampling\n",
    "        print(\"    Running sparse preprocessing...\")\n",
    "        sparse_result = run_sparse_rw_sampling(data, rw_seed, n_nodes)\n",
    "        sparse_file_size = save_step_matrices(step_matrices_dir, 'sparse', n_nodes, rw_seed, \n",
    "                                            sparse_result['step_matrices'], config)\n",
    "        \n",
    "        # Run dense sampling if applicable\n",
    "        if run_dense:\n",
    "            print(\"    Running dense preprocessing...\")\n",
    "            dense_result = run_dense_rw_sampling(data, rw_seed, n_nodes)\n",
    "            dense_file_size = save_step_matrices(step_matrices_dir, 'dense', n_nodes, rw_seed,\n",
    "                                               dense_result['step_matrices'], config)\n",
    "        else:\n",
    "            print(\"    Skipping dense (graph too large)\")\n",
    "            dense_result = None\n",
    "            dense_file_size = None\n",
    "        \n",
    "        # Compile statistics\n",
    "        stat_entry = {\n",
    "            'n_nodes': n_nodes,\n",
    "            'n_edges': data['A_sparse'].nnz // 2,\n",
    "            'seed': rw_seed,\n",
    "            'sparse_rw_time': sparse_result['time'],\n",
    "            'dense_rw_time': dense_result['time'] if dense_result else None,\n",
    "            'sparse_size_mb': sparse_result['size_mb'],\n",
    "            'dense_size_mb': dense_result['size_mb'] if dense_result else None,\n",
    "            'sparse_dense_equiv_mb': sparse_result['dense_equiv_mb'],\n",
    "            'compression_ratio': (dense_result['size_mb'] / sparse_result['size_mb'] \n",
    "                                if dense_result and sparse_result['size_mb'] > 0 else np.nan),\n",
    "            'time_speedup': (dense_result['time'] / sparse_result['time'] \n",
    "                           if dense_result else np.nan),\n",
    "            'sparse_file_size_mb': sparse_file_size,\n",
    "            'dense_file_size_mb': dense_file_size,\n",
    "            'sparse_total_nnz': sparse_result['total_nnz'],\n",
    "            'sparse_avg_nnz_per_matrix': sparse_result['avg_nnz_per_matrix'],\n",
    "            'graph_sparsity': data['A_sparse'].nnz / (n_nodes**2),\n",
    "            'step_matrix_sparsity': sparse_result['sparsity'],\n",
    "            'run_dense': run_dense\n",
    "        }\n",
    "        results.append(stat_entry)\n",
    "        \n",
    "        # Cleanup\n",
    "        del sparse_result\n",
    "        if dense_result:\n",
    "            del dense_result\n",
    "        if n_nodes >= 1000:\n",
    "            gc.collect()\n",
    "    \n",
    "    # Cleanup data\n",
    "    del data\n",
    "    if n_nodes >= 1000:\n",
    "        gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_summary_statistics(rw_df):\n",
    "    \"\"\"Print formatted summary statistics\"\"\"\n",
    "    comparison_df = rw_df[rw_df['run_dense'] == True]\n",
    "    sparse_only_df = rw_df[rw_df['run_dense'] == False]\n",
    "    \n",
    "    if len(comparison_df) > 0:\n",
    "        summary = comparison_df.groupby('n_nodes').agg({\n",
    "            'sparse_rw_time': ['mean', 'std'],\n",
    "            'dense_rw_time': ['mean', 'std'],\n",
    "            'time_speedup': ['mean', 'std'],\n",
    "            'sparse_size_mb': ['mean', 'std'], \n",
    "            'dense_size_mb': ['mean', 'std'],\n",
    "            'compression_ratio': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        print(f\"\\nüìä Dense vs Sparse Comparison:\")\n",
    "        print(summary)\n",
    "    \n",
    "    if len(sparse_only_df) > 0:\n",
    "        sparse_summary = sparse_only_df.groupby('n_nodes').agg({\n",
    "            'sparse_rw_time': ['mean', 'std'],\n",
    "            'sparse_size_mb': ['mean', 'std'],\n",
    "            'sparse_file_size_mb': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        print(f\"\\nüìä Sparse-Only Results (Large Graphs):\")\n",
    "        print(sparse_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c0fb8",
   "metadata": {},
   "source": [
    "#### Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c1bc62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RW sampling experiments for 6 graph sizes with 5 seeds each...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 32 nodes...\n",
      "Loading cached data for 32 nodes...\n",
      "  Seed 1/5 (seed=42)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 2/5 (seed=43)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 3/5 (seed=44)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 4/5 (seed=45)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 5/5 (seed=46)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  17%|‚ñà‚ñã        | 1/6 [00:04<00:21,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 64 nodes...\n",
      "Loading cached data for 64 nodes...\n",
      "  Seed 1/5 (seed=42)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 2/5 (seed=43)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 3/5 (seed=44)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 4/5 (seed=45)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 5/5 (seed=46)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:09<00:18,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 128 nodes...\n",
      "Loading cached data for 128 nodes...\n",
      "  Seed 1/5 (seed=42)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 2/5 (seed=43)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 3/5 (seed=44)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 4/5 (seed=45)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 5/5 (seed=46)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:15<00:15,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 256 nodes...\n",
      "Loading cached data for 256 nodes...\n",
      "  Seed 1/5 (seed=42)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 2/5 (seed=43)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 3/5 (seed=44)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 4/5 (seed=45)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n",
      "  Seed 5/5 (seed=46)\n",
      "    Running sparse preprocessing...\n",
      "    Running dense preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:24<00:13,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 512 nodes...\n",
      "Loading cached data for 512 nodes...\n",
      "  Seed 1/5 (seed=42)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 2/5 (seed=43)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 3/5 (seed=44)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 4/5 (seed=45)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 5/5 (seed=46)\n",
      "    Running sparse preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:27<00:05,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Skipping dense (graph too large)\n",
      "\n",
      "Processing 1024 nodes...\n",
      "Loading cached data for 1024 nodes...\n",
      "  Seed 1/5 (seed=42)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 2/5 (seed=43)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 3/5 (seed=44)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 4/5 (seed=45)\n",
      "    Running sparse preprocessing...\n",
      "    Skipping dense (graph too large)\n",
      "  Seed 5/5 (seed=46)\n",
      "    Running sparse preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:34<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Skipping dense (graph too large)\n",
      "\n",
      "‚úÖ RW sampling complete!\n",
      "   Step matrices saved to: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/step_matrices\n",
      "   Statistics saved to: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/stats/rw_sampling_stats.csv\n",
      "   Processed 30 experiments\n",
      "\n",
      "üìä Dense vs Sparse Comparison:\n",
      "        sparse_rw_time        dense_rw_time        time_speedup         \\\n",
      "                  mean    std          mean    std         mean    std   \n",
      "n_nodes                                                                  \n",
      "32               0.348  0.007         0.108  0.001        0.309  0.007   \n",
      "64               0.360  0.011         0.200  0.009        0.555  0.035   \n",
      "128              0.384  0.012         0.407  0.021        1.060  0.078   \n",
      "256              0.454  0.014         0.891  0.017        1.966  0.056   \n",
      "\n",
      "        sparse_size_mb      dense_size_mb      compression_ratio       \n",
      "                  mean  std          mean  std              mean  std  \n",
      "n_nodes                                                                \n",
      "32               0.004  0.0         0.023  0.0             5.333  0.0  \n",
      "64               0.009  0.0         0.094  0.0            10.667  0.0  \n",
      "128              0.018  0.0         0.375  0.0            21.333  0.0  \n",
      "256              0.035  0.0         1.500  0.0            42.667  0.0  \n",
      "\n",
      "üìä Sparse-Only Results (Large Graphs):\n",
      "        sparse_rw_time        sparse_size_mb      sparse_file_size_mb     \n",
      "                  mean    std           mean  std                mean  std\n",
      "n_nodes                                                                   \n",
      "512              0.546  0.012          0.070  0.0               0.059  0.0\n",
      "1024             0.740  0.015          0.141  0.0               0.118  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_rw_sampling_experiment():\n",
    "    \"\"\"Run random walk sampling experiments for all graph sizes and seeds\"\"\"\n",
    "    \n",
    "    # Setup directories\n",
    "    step_matrices_dir = os.path.join(os.getcwd(), 'step_matrices')\n",
    "    stats_dir = os.path.join(os.getcwd(), 'stats')\n",
    "    os.makedirs(step_matrices_dir, exist_ok=True)\n",
    "    os.makedirs(stats_dir, exist_ok=True)\n",
    "    \n",
    "    # Process all graph sizes\n",
    "    all_sizes = GP_GRAPH_SIZES + GP_SPARSE_ONLY_SIZES\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Running RW sampling experiments for {len(all_sizes)} graph sizes with {N_REPEATS} seeds each...\")\n",
    "    \n",
    "    for n_nodes in tqdm(all_sizes, desc=\"Graph sizes\"):\n",
    "        graph_results = process_single_graph(n_nodes, DATA_DIR, step_matrices_dir, RW_SEEDS)\n",
    "        all_results.extend(graph_results)\n",
    "    \n",
    "    # Save and summarize results\n",
    "    rw_df = pd.DataFrame(all_results)\n",
    "    stats_file = os.path.join(stats_dir, 'rw_sampling_stats.csv')\n",
    "    rw_df.to_csv(stats_file, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ RW sampling complete!\")\n",
    "    print(f\"   Step matrices saved to: {step_matrices_dir}\")\n",
    "    print(f\"   Statistics saved to: {stats_file}\")\n",
    "    print(f\"   Processed {len(all_results)} experiments\")\n",
    "    \n",
    "    print_summary_statistics(rw_df)\n",
    "    return rw_df\n",
    "\n",
    "# Run the experiment\n",
    "rw_results_df = run_rw_sampling_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5ebcf",
   "metadata": {},
   "source": [
    "### Sparse GP Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8003cf",
   "metadata": {},
   "source": [
    "#### Sparse Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55ff0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-accelerated Sparse GP Model\n",
    "class SparseGraphGPModel(gpytorch.models.ExactGP):\n",
    "    \"\"\"Sparse Graph GP Model with pathwise conditioning prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, x_train, y_train, likelihood, step_matrices_torch):\n",
    "        super().__init__(x_train, y_train, likelihood)\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = SparseGRFKernel(\n",
    "            max_walk_length=MAX_WALK_LENGTH, \n",
    "            step_matrices_torch=step_matrices_torch\n",
    "        )\n",
    "        self.num_nodes = step_matrices_torch[0].shape[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def predict(self, x_test, n_samples=64):\n",
    "        \"\"\"\n",
    "        Batch pathwise conditioning prediction\n",
    "        \n",
    "        f_test_posterior = f_test_prior + K_test_train @ v\n",
    "        v = (K_train_train + noise_variance*I)^{-1} @ (y_train - (f_train_prior + eps))\n",
    "        \"\"\"\n",
    "        num_train = self.x_train.shape[0]\n",
    "        train_indices = self.x_train.int().flatten()\n",
    "        test_indices = x_test.int().flatten()\n",
    "        \n",
    "        # Feature matrices\n",
    "        phi = self.covar_module._get_feature_matrix()\n",
    "        phi_train = phi[train_indices, :]\n",
    "        phi_test = phi[test_indices, :]\n",
    "        \n",
    "        # Covariance matrices\n",
    "        K_train_train = phi_train @ phi_train.T\n",
    "        K_test_train = phi_test @ phi_train.T\n",
    "        \n",
    "        # Noise setup\n",
    "        noise_variance = self.likelihood.noise.item()\n",
    "        noise_std = torch.sqrt(torch.tensor(noise_variance, device=x_test.device))\n",
    "        A = K_train_train + noise_variance * IdentityLinearOperator(num_train, device=x_test.device)\n",
    "        \n",
    "        # Batch samples\n",
    "        eps1_batch = torch.randn(n_samples, self.num_nodes, device=x_test.device)\n",
    "        eps2_batch = noise_std * torch.randn(n_samples, num_train, device=x_test.device)\n",
    "        \n",
    "        # Prior samples\n",
    "        f_test_prior_batch = eps1_batch @ phi_test.T\n",
    "        f_train_prior_batch = eps1_batch @ phi_train.T\n",
    "        \n",
    "        # CG solve\n",
    "        b_batch = self.y_train.unsqueeze(0) - (f_train_prior_batch + eps2_batch)\n",
    "        v_batch = linear_cg(A._matmul, b_batch.T, tolerance=gsettings.cg_tolerance.value())\n",
    "        \n",
    "        # Posterior\n",
    "        return f_test_prior_batch + (K_test_train @ v_batch).T\n",
    "\n",
    "def load_step_matrices_from_file(filepath, device):\n",
    "    \"\"\"Load and convert step matrices to torch tensors\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    step_matrices_scipy = data['step_matrices_torch']  # These are actually scipy matrices\n",
    "    \n",
    "    # Convert to torch tensors using GraphPreprocessor's static method\n",
    "    step_matrices_torch = []\n",
    "    for mat in step_matrices_scipy:\n",
    "        tensor = GraphPreprocessor.from_scipy_csr(mat).to(device)\n",
    "        step_matrices_torch.append(SparseLinearOperator(tensor))\n",
    "    \n",
    "    return step_matrices_torch\n",
    "\n",
    "def train_sparse_gp(data_torch, step_matrices_torch, n_epochs=50, lr=0.1):\n",
    "    \"\"\"Train sparse GP model with GPU\"\"\"\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(output_device)\n",
    "    model = SparseGraphGPModel(\n",
    "        data_torch['X_train'], data_torch['y_train'], \n",
    "        likelihood, step_matrices_torch\n",
    "    ).to(output_device)\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    train_time_start = time.time()\n",
    "    for i in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_torch['X_train'])\n",
    "        loss = -mll(output, data_torch['y_train'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_time = time.time() - train_time_start\n",
    "    \n",
    "    return model, likelihood, train_time\n",
    "\n",
    "def evaluate_sparse_gp(model, likelihood, data_torch, n_samples=64):\n",
    "    \"\"\"Evaluate sparse GP with pathwise sampling\"\"\"\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    inference_time_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        test_samples = model.predict(data_torch['X_test'], n_samples)\n",
    "        test_mean = test_samples.mean(dim=0)\n",
    "        test_std = test_samples.std(dim=0)\n",
    "    inference_time = time.time() - inference_time_start\n",
    "    \n",
    "    test_rmse = torch.sqrt(torch.mean((data_torch['y_test'] - test_mean) ** 2)).item()\n",
    "    \n",
    "    return {\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mean': test_mean,\n",
    "        'test_std': test_std, \n",
    "        'inference_time': inference_time,\n",
    "        'noise_variance': likelihood.noise.item(),\n",
    "        'modulator': model.covar_module.modulator_vector.detach().cpu().numpy()\n",
    "    }\n",
    "\n",
    "def run_sparse_gp_experiment(n_nodes, rw_seed, n_epochs=N_EPOCHS):\n",
    "    \"\"\"Run complete sparse GP experiment for one graph size and seed\"\"\"\n",
    "    print(f\"  Processing {n_nodes} nodes, seed {rw_seed}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = generate_and_cache_data(n_nodes, DATA_DIR, **DATA_SYNTHESIS_PARAMS)\n",
    "    \n",
    "    # Convert to torch tensors on GPU\n",
    "    data_torch = {\n",
    "        'X_train': torch.tensor(data['train_idx'], dtype=torch.float32, device=output_device).unsqueeze(1),\n",
    "        'y_train': torch.tensor(data['y_train'].flatten(), dtype=torch.float32, device=output_device),\n",
    "        'X_test': torch.tensor(data['test_idx'], dtype=torch.float32, device=output_device).unsqueeze(1),\n",
    "        'y_test': torch.tensor(data['y_test'].flatten(), dtype=torch.float32, device=output_device)\n",
    "    }\n",
    "    \n",
    "    # Load step matrices\n",
    "    step_matrices_file = os.path.join(os.getcwd(), 'step_matrices', f'step_matrices_sparse_n{n_nodes}_seed{rw_seed}.pkl')\n",
    "    if not os.path.exists(step_matrices_file):\n",
    "        print(f\"    Step matrices not found: {step_matrices_file}\")\n",
    "        return None\n",
    "    \n",
    "    step_matrices_torch = load_step_matrices_from_file(step_matrices_file, output_device)\n",
    "    \n",
    "    # Train model\n",
    "    model, likelihood, train_time = train_sparse_gp(data_torch, step_matrices_torch, n_epochs, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_results = evaluate_sparse_gp(model, likelihood, data_torch)\n",
    "    \n",
    "    return {\n",
    "        'n_nodes': n_nodes,\n",
    "        'seed': rw_seed,\n",
    "        'n_train': len(data['train_idx']),\n",
    "        'n_test': len(data['test_idx']),\n",
    "        'train_time': train_time, \n",
    "        'inference_time': eval_results['inference_time'],\n",
    "        'total_time': train_time + eval_results['inference_time'],\n",
    "        'test_rmse': eval_results['test_rmse'],\n",
    "        'noise_variance': eval_results['noise_variance'],\n",
    "        'modulator_l2': np.linalg.norm(eval_results['modulator'])\n",
    "    }\n",
    "\n",
    "def run_sparse_gp_scaling_experiment():\n",
    "    \"\"\"Run sparse GP experiments across all sizes and seeds\"\"\"\n",
    "    gp_results = []\n",
    "    all_sizes = GP_GRAPH_SIZES + GP_SPARSE_ONLY_SIZES\n",
    "    \n",
    "    print(f\"Running sparse GP experiments for {len(all_sizes)} sizes √ó {len(RW_SEEDS)} seeds...\")\n",
    "    \n",
    "    for n_nodes in tqdm(all_sizes, desc=\"Graph sizes\"):\n",
    "        for rw_seed in RW_SEEDS:\n",
    "            result = run_sparse_gp_experiment(n_nodes, rw_seed)\n",
    "            if result:\n",
    "                gp_results.append(result)\n",
    "            \n",
    "            # Cleanup GPU memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Save results using helper function\n",
    "    gp_df = pd.DataFrame(gp_results)\n",
    "    stats_dir = os.path.join(os.getcwd(), 'stats') \n",
    "    \n",
    "    config_params = {\n",
    "        'graph_sizes': sorted(gp_df['n_nodes'].unique().tolist()),\n",
    "        'seeds': sorted(gp_df['seed'].unique().tolist()),\n",
    "        'n_epochs': N_EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'walks_per_node': WALKS_PER_NODE,\n",
    "        'p_halt': P_HALT,\n",
    "        'max_walk_length': MAX_WALK_LENGTH,\n",
    "        'device': str(output_device)\n",
    "    }\n",
    "    \n",
    "    save_experiment_results(gp_df, 'sparse_gp_scaling', stats_dir, config_params)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Sparse GP scaling complete! Processed {len(gp_results)} experiments\")\n",
    "    return gp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61feca",
   "metadata": {},
   "source": [
    "#### Run it!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eac5dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sparse GP experiments for 6 sizes √ó 5 seeds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 32 nodes, seed 42\n",
      "Loading cached data for 32 nodes...\n",
      "  Processing 32 nodes, seed 43\n",
      "Loading cached data for 32 nodes...\n",
      "  Processing 32 nodes, seed 44\n",
      "Loading cached data for 32 nodes...\n",
      "  Processing 32 nodes, seed 45\n",
      "Loading cached data for 32 nodes...\n",
      "  Processing 32 nodes, seed 46\n",
      "Loading cached data for 32 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  17%|‚ñà‚ñã        | 1/6 [00:29<02:26, 29.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 64 nodes, seed 42\n",
      "Loading cached data for 64 nodes...\n",
      "  Processing 64 nodes, seed 43\n",
      "Loading cached data for 64 nodes...\n",
      "  Processing 64 nodes, seed 44\n",
      "Loading cached data for 64 nodes...\n",
      "  Processing 64 nodes, seed 45\n",
      "Loading cached data for 64 nodes...\n",
      "  Processing 64 nodes, seed 46\n",
      "Loading cached data for 64 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [01:09<02:22, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 128 nodes, seed 42\n",
      "Loading cached data for 128 nodes...\n",
      "  Processing 128 nodes, seed 43\n",
      "Loading cached data for 128 nodes...\n",
      "  Processing 128 nodes, seed 44\n",
      "Loading cached data for 128 nodes...\n",
      "  Processing 128 nodes, seed 45\n",
      "Loading cached data for 128 nodes...\n",
      "  Processing 128 nodes, seed 46\n",
      "Loading cached data for 128 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [02:00<02:08, 42.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 256 nodes, seed 42\n",
      "Loading cached data for 256 nodes...\n",
      "  Processing 256 nodes, seed 43\n",
      "Loading cached data for 256 nodes...\n",
      "  Processing 256 nodes, seed 44\n",
      "Loading cached data for 256 nodes...\n",
      "  Processing 256 nodes, seed 45\n",
      "Loading cached data for 256 nodes...\n",
      "  Processing 256 nodes, seed 46\n",
      "Loading cached data for 256 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [02:56<01:35, 47.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 512 nodes, seed 42\n",
      "Loading cached data for 512 nodes...\n",
      "  Processing 512 nodes, seed 43\n",
      "Loading cached data for 512 nodes...\n",
      "  Processing 512 nodes, seed 44\n",
      "Loading cached data for 512 nodes...\n",
      "  Processing 512 nodes, seed 45\n",
      "Loading cached data for 512 nodes...\n",
      "  Processing 512 nodes, seed 46\n",
      "Loading cached data for 512 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [03:47<00:49, 49.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 1024 nodes, seed 42\n",
      "Loading cached data for 1024 nodes...\n",
      "  Processing 1024 nodes, seed 43\n",
      "Loading cached data for 1024 nodes...\n",
      "  Processing 1024 nodes, seed 44\n",
      "Loading cached data for 1024 nodes...\n",
      "  Processing 1024 nodes, seed 45\n",
      "Loading cached data for 1024 nodes...\n",
      "  Processing 1024 nodes, seed 46\n",
      "Loading cached data for 1024 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:42<00:00, 47.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ sparse_gp_scaling results saved:\n",
      "   Main file: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/stats/sparse_gp_scaling_stats.csv\n",
      "   Timestamped: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/stats/sparse_gp_scaling_stats_20250809_235219.csv\n",
      "   Config: sparse_gp_scaling_config_20250809_235219.json\n",
      "   Summary: sparse_gp_scaling_summary_20250809_235219.csv\n",
      "\n",
      "‚úÖ Sparse GP scaling complete! Processed 30 experiments\n"
     ]
    }
   ],
   "source": [
    "# Run sparse GP scaling experiment\n",
    "sparse_gp_results_df = run_sparse_gp_scaling_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337be97",
   "metadata": {},
   "source": [
    "### Dense GP Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f2551",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7367e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense GP Model using GPflow\n",
    "class DenseGPModel:\n",
    "    \"\"\"Dense GP model using GPflow\"\"\"\n",
    "    \n",
    "    def __init__(self, data, step_matrices_dense=None, walks_per_node=WALKS_PER_NODE, p_halt=P_HALT, max_walk_length=MAX_WALK_LENGTH):\n",
    "        self.data = data\n",
    "        self.step_matrices_dense = step_matrices_dense\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.p_halt = p_halt\n",
    "        self.max_walk_length = max_walk_length\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build GPflow model with dense kernel\"\"\"\n",
    "        # Create dense kernel with optional pre-computed step matrices\n",
    "        self.kernel = GraphGeneralFastGRFKernel(\n",
    "            self.data['A_dense'], \n",
    "            walks_per_node=self.walks_per_node, \n",
    "            p_halt=self.p_halt, \n",
    "            max_walk_length=self.max_walk_length,\n",
    "            step_matrices=self.step_matrices_dense,  # Pre-computed step matrices\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        \n",
    "        # Create GPflow model\n",
    "        self.model = gpflow.models.GPR(\n",
    "            data=(self.data['X_train'], self.data['y_train']), \n",
    "            kernel=self.kernel, \n",
    "            noise_variance=INITIAL_NOISE_VARIANCE\n",
    "        )\n",
    "        \n",
    "    def train(self, n_epochs=50):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        optimizer = gpflow.optimizers.Scipy()\n",
    "        \n",
    "        def objective():\n",
    "            return -self.model.log_marginal_likelihood()\n",
    "        \n",
    "        optimizer.minimize(\n",
    "            objective,\n",
    "            self.model.trainable_variables,\n",
    "            options={\"maxiter\": n_epochs},\n",
    "            compile=False\n",
    "        )\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        mean_pred, var_pred = self.model.predict_f(X_test)\n",
    "        return mean_pred.numpy(), var_pred.numpy()\n",
    "\n",
    "def train_dense_gp(data, step_matrices_dense=None, n_epochs=50):\n",
    "    \"\"\"Train dense GP model\"\"\"\n",
    "    model_wrapper = DenseGPModel(data, step_matrices_dense)\n",
    "    \n",
    "    # Build model (kernel initialization time)\n",
    "    model_wrapper.build_model()\n",
    "    \n",
    "    # Train model\n",
    "    train_time_start = time.time()\n",
    "    model_wrapper.train(n_epochs)\n",
    "    train_time = time.time() - train_time_start\n",
    "    \n",
    "    return model_wrapper, train_time\n",
    "\n",
    "def evaluate_dense_gp(model_wrapper, data):\n",
    "    \"\"\"Evaluate dense GP model\"\"\"\n",
    "    inference_time_start = time.time()\n",
    "    test_mean, test_var = model_wrapper.predict(data['X_test'])\n",
    "    inference_time = time.time() - inference_time_start\n",
    "    \n",
    "    test_std = np.sqrt(test_var.flatten())\n",
    "    test_rmse = np.sqrt(mean_squared_error(data['y_test'], test_mean))\n",
    "    \n",
    "    return {\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mean': test_mean.flatten(),\n",
    "        'test_std': test_std,\n",
    "        'inference_time': inference_time,\n",
    "        'noise_variance': float(model_wrapper.model.likelihood.variance.numpy()),\n",
    "        'modulator': model_wrapper.kernel.modulator_vector.numpy()\n",
    "    }\n",
    "\n",
    "def run_dense_gp_experiment(n_nodes, rw_seed, n_epochs=N_EPOCHS):\n",
    "    \"\"\"Run complete dense GP experiment for one graph size and seed\"\"\"\n",
    "    print(f\"  Processing {n_nodes} nodes, seed {rw_seed}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = generate_and_cache_data(n_nodes, DATA_DIR, **DATA_SYNTHESIS_PARAMS)\n",
    "    \n",
    "    # Load dense step matrices if available\n",
    "    step_matrices_file = os.path.join(os.getcwd(), 'step_matrices', f'step_matrices_dense_n{n_nodes}_seed{rw_seed}.pkl')\n",
    "    step_matrices_dense = None\n",
    "    \n",
    "    if os.path.exists(step_matrices_file):\n",
    "        with open(step_matrices_file, 'rb') as f:\n",
    "            step_data = pickle.load(f)\n",
    "            step_matrices_dense = step_data['step_matrices']\n",
    "        print(f\"    Loaded pre-computed dense step matrices\")\n",
    "    else:\n",
    "        print(f\"    Computing dense step matrices on-the-fly\")\n",
    "    \n",
    "    # Train model\n",
    "    model_wrapper, train_time = train_dense_gp(data, step_matrices_dense, n_epochs)\n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_results = evaluate_dense_gp(model_wrapper, data)\n",
    "    \n",
    "    return {\n",
    "        'n_nodes': n_nodes,\n",
    "        'seed': rw_seed,\n",
    "        'n_train': len(data['train_idx']),\n",
    "        'n_test': len(data['test_idx']),\n",
    "        'train_time': train_time,\n",
    "        'inference_time': eval_results['inference_time'],\n",
    "        'total_time': train_time + eval_results['inference_time'],\n",
    "        'test_rmse': eval_results['test_rmse'],\n",
    "        'noise_variance': eval_results['noise_variance'],\n",
    "        'modulator_l2': np.linalg.norm(eval_results['modulator'])\n",
    "    }\n",
    "\n",
    "def run_dense_gp_scaling_experiment():\n",
    "    \"\"\"Run dense GP experiments across all feasible sizes and seeds\"\"\"\n",
    "    gp_results = []\n",
    "    \n",
    "    # Run on the original GP_GRAPH_SIZES (no size restrictions)\n",
    "    feasible_sizes = GP_GRAPH_SIZES\n",
    "    \n",
    "    print(f\"Running dense GP experiments for {len(feasible_sizes)} sizes √ó {len(RW_SEEDS)} seeds...\")\n",
    "    print(f\"Sizes: {feasible_sizes}\")\n",
    "    \n",
    "    for n_nodes in tqdm(feasible_sizes, desc=\"Graph sizes\"):\n",
    "        for rw_seed in RW_SEEDS:\n",
    "            result = run_dense_gp_experiment(n_nodes, rw_seed)\n",
    "            if result:\n",
    "                gp_results.append(result)\n",
    "            \n",
    "            # Cleanup memory\n",
    "            gc.collect()\n",
    "    \n",
    "    # Save results using helper function\n",
    "    if gp_results:\n",
    "        gp_df = pd.DataFrame(gp_results)\n",
    "        stats_dir = os.path.join(os.getcwd(), 'stats')\n",
    "        \n",
    "        config_params = {\n",
    "            'graph_sizes': sorted(gp_df['n_nodes'].unique().tolist()),\n",
    "            'seeds': sorted(gp_df['seed'].unique().tolist()),\n",
    "            'n_epochs': N_EPOCHS,\n",
    "            'walks_per_node': WALKS_PER_NODE,\n",
    "            'p_halt': P_HALT,\n",
    "            'max_walk_length': MAX_WALK_LENGTH,\n",
    "            'framework': 'gpflow_dense'\n",
    "        }\n",
    "        \n",
    "        save_experiment_results(gp_df, 'dense_gp_scaling', stats_dir, config_params)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dense GP scaling complete! Processed {len(gp_results)} experiments\")\n",
    "        return gp_df\n",
    "    else:\n",
    "        print(\"\\n‚ùå No dense GP experiments completed successfully\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f87764f",
   "metadata": {},
   "source": [
    "#### Run it!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7503aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dense GP experiments for 3 sizes √ó 5 seeds...\n",
      "Sizes: [32, 64, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 32 nodes, seed 42\n",
      "Loading cached data for 32 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 32 nodes, seed 43\n",
      "Loading cached data for 32 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 32 nodes, seed 44\n",
      "Loading cached data for 32 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 32 nodes, seed 45\n",
      "Loading cached data for 32 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 32 nodes, seed 46\n",
      "Loading cached data for 32 nodes...\n",
      "    Loaded pre-computed dense step matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:10<00:20, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 64 nodes, seed 42\n",
      "Loading cached data for 64 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 64 nodes, seed 43\n",
      "Loading cached data for 64 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 64 nodes, seed 44\n",
      "Loading cached data for 64 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 64 nodes, seed 45\n",
      "Loading cached data for 64 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 64 nodes, seed 46\n",
      "Loading cached data for 64 nodes...\n",
      "    Loaded pre-computed dense step matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:18<00:09,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing 128 nodes, seed 42\n",
      "Loading cached data for 128 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 128 nodes, seed 43\n",
      "Loading cached data for 128 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 128 nodes, seed 44\n",
      "Loading cached data for 128 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 128 nodes, seed 45\n",
      "Loading cached data for 128 nodes...\n",
      "    Loaded pre-computed dense step matrices\n",
      "  Processing 128 nodes, seed 46\n",
      "Loading cached data for 128 nodes...\n",
      "    Loaded pre-computed dense step matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph sizes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:26<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ dense_gp_scaling results saved:\n",
      "   Main file: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/stats/dense_gp_scaling_stats.csv\n",
      "   Timestamped: /scratches/cartwright/mz473/Efficient-Gaussian-Process-on-Graphs/experiments_sparse/scaling_exp/stats/dense_gp_scaling_stats_20250809_235600.csv\n",
      "   Config: dense_gp_scaling_config_20250809_235600.json\n",
      "   Summary: dense_gp_scaling_summary_20250809_235600.csv\n",
      "\n",
      "‚úÖ Dense GP scaling complete! Processed 15 experiments\n"
     ]
    }
   ],
   "source": [
    "# Run dense GP scaling experiment  \n",
    "dense_gp_results_df = run_dense_gp_scaling_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01395a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
